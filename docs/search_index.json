[["index.html", "High clonal diversity and spatial genetic admixture in early prostate cancer and surrounding normal tissue 1 Introduction", " High clonal diversity and spatial genetic admixture in early prostate cancer and surrounding normal tissue Last compiled on 2023-07-13 1 Introduction On this page you can find all code associated with the manuscript ‘High clonal diversity and spatial genetic admixture in early prostate cancer and surrounding normal tissue’. All the individual R Markdown files can be found in the github repository. If you wish to run any of the code yourself, simply clone the repository and (interactively) run the R Markdown files. Due to the size of the processed data, the data can be downloaded from figshare. Simply download the data and extract it in your cloned repository. Finally, due to changes in patient labeling, the patient numbering used in the scripts and in the datasets is incremented by 1 compared to the patient numbering in the manuscript. E.g., Patient (P) 2 in the manuscript will be P3 here. "],["preprocessing.html", "2 Preprocessing 2.1 Demultiplexing of fastq files 2.2 Adapter trimming 2.3 Aligning of cell specific fastq files 2.4 Moving barcodes and UMIs to bam tags 2.5 Deduplication 2.6 QC summary", " 2 Preprocessing 2.1 Demultiplexing of fastq files Because scCUTseq libraries consist of, typically, 384 or 96 cells, we need to demultiplex the fastq files based on the cellular barcodes that are contained within the reads. We do this using a custom python script. Briefly, we extract the cellular barcode and the UMI and move this information to the read name. Unfortunately, we are not able to share the raw fastq files but the two Python scripts used can be found here. This script can be called through the command line and an example of this is as follows: processBarcode.py -i {input_fastq} -o {output_directory} -b {list_of_barcodes} -d {number_of_mismatches} -p {barcoding_pattern} -t {threads} -r {sequencing_type} -v Below you can find a brief explanation of the different parameters: input_fastq: the path to the input fastq file output_directory: the path to the output directory list of barcodes: a text file with each row containing one cellular barcode, an example of this file can be found here. number_of_mismatches: denotes the number of mismatches that a barcode can have to still be assigned to one of the cellular barcodes, this is by default 1 or 2 depending on the length of the barcodes (8 or 11 in our libraries). barcoding_pattern: The pattern of the barcode/UMI in the reads. This is a character string consisting of U/B/D, denoting which base belongs to the UMI (U), Barcode (B) and recognition site (D). In the case of our 96 and 384 cell libraries either UUUUUUUUBBBBBBBBDDDD or UUUUUUUUBBBBBBBBBBBDDDD, respectively. threads: Number of threads to use for parallel processing sequencing_type: Either ‘single’ or ‘paired’ depending on sequencing type v: Verbose processing 2.2 Adapter trimming In the case of paired-end sequencing on fragments that are too short, we do additional adapter trimming. We use fastp for this, which is described here. An example of the command used is as follows: fastp -i {input.fastq1} -I {input.fastq2} -o {output.fastq1} -O {output.fastq2} --detect_adapter_for_pe --cut_front --cut_tail --cut_mean_quality 30 2.3 Aligning of cell specific fastq files Following the demultiplexing (and potential trimming) we align the cell specific fastq files to the reference genome, in our case GRCh37. We perform the alignment using bwa-mem, piping it into samtools sort to save time and disk space. The call used is as follows: bwa mem -M -t {threads} -R {readgroup_info} {input_fastqs} {reference_file} | \\ samtools sort -o {output_bam} &amp;&amp; samtools index -@ {threads} {output_bam} 2.4 Moving barcodes and UMIs to bam tags After alignment we move the UMI and barcode information, which is stored in the read name, to the bam tags. We do this using a custom python script which can be found here. This script is also called through bash as follows: moveBamtags.py -i {input_bam} | samtools view -hbo {output_bam} &amp;&amp; samtools index {output_bam} 2.5 Deduplication To deduplicate the bam files we use umi_tools which takes advantage of the fact that we have UMIs incorporated in our sequencing reads. Once again, this script is called through bash and you can find an example of this below: umi_tools dedup -I {input_bam} -S {output_bam} -L {output_log} --extract-umi-method tag --umi-tag &#39;RX:Z&#39; --mapping-quality 30 &amp;&amp; samtools index -@ {threads} {output_bam} You can find the documentation of umi-tools describing all options here. 2.6 QC summary To make a QC summary report we use a tool called alfred which can be found here. We run this before and after deduplication as follows: alfred qc -r {input_ref} -o {output} {input_bam} Following this we summarize the output of both files in an tsv file using the following bash call zgrep ^ME {prededup_bamfile} | cut -f 2- | sed -n &#39;1p;0~2p&#39; &gt; {outdir}/all.tsv zgrep ^ME {postdedup_bamfile} | cut -f 2- | sed -n &#39;1p;0~2p&#39; &gt; {outdir}/dedup.tsv The entire preprocessing workflow is automated and available as a snakemake file and is available in the github repository here. "],["copy-number-calling.html", "3 Copy number calling 3.1 Transform bam files 3.2 Count reads 3.3 Calling copy numbers 3.4 Generate plots", " 3 Copy number calling 3.1 Transform bam files To be able to efficiently count reads in (variable width) genomic windows, we first transform bam files into bed files. We do this using the bamtobed function from bedtools. We make sure we only select reads that have a high mapping quality and are properly mapped (and paired in the case of paired-end sequencing). We only select chromosomes 1-22 and X and make sure the the bed file is properly sorted afterwards. We call this as follows: samtools view -q 30 -F 1024 -b {input} | bedtools bamtobed | grep -E &#39;^[0-9]|^X|^chr[0-9]|^chrX&#39; |\\ sort -k1,1V -k2,2n -k3,3n | gzip -c &gt; {output} 3.2 Count reads We then use a file containing variable width bins, which can be found in the github repository here, and count the reads in each genomic region. We use bedtools intersect to do this. We run this per sample and then combine all the samples (normally all 384 cells from one library) into one large file. echo {sample_name} &gt; {output} bedtools intersect -nonamecheck -F 0.5 -sorted -c -a {variable_width_bin_file} -b {input.bed} | cut -f4 &gt;&gt; {output} paste {input_files} | gzip &gt; {output_file} 3.3 Calling copy numbers The above generated files are then used to call copy numbers. We use a custom script to call copy number profiles and there are, depending on the use-case, different ways to run this script. Below is an example call and we will explain the parameters used after that. Rscript cnv_calling.R --counts {counts} --bins {bins} --binsize {binsize} --blacklist {blacklist} --normseg {normseg} --gc {gc_file} --segmentation {segmentation_type} --penalty {segmentation_penalty} --type single --randomforest {path_to_randomforest_model} --rfthreshold {rf_threshold} --removethreshold {removethreshold} --minploidy {minploidy} --maxploidy {maxploidy} --minpurity {minpurity} --maxpurity {maxpurity} --sex {sex} --threads {threads} --output {output} Below you can find a brief explanation of the different parameters: counts: file containing binned counts bins: file containing bins binsize: integer denoting binsize used blacklist: file containing regions that should be filtered out (telomeric/centromeric regions for instance) normseg: additional normalization of regions if required segmentation: either ‘joint’ or ‘single’ for multipcf or CBS segmentation, respectively. penalty: penalty threshold used for joint segmentation. If using single segmentation this should be replaced by --prune {prune_penalty} and --alpha {alpha_penalty} type: ‘single’ or ‘bulk’ depending on if it’s single-cell sequencing or bulk sequencing randomforest: path to the randomforest model rfthreshold: threshold used for randomforest classification minploidy, maxploidy, minpurity and maxpurity: minimum and maximum purity/ploidies for grid search sex: either male or female depending on sex of the sample sequenced threads: number of threads to use for parallel processing output: output directory 3.4 Generate plots Finally, we generate some profile plots and genomewide heatmaps based on the accomplished copy number calling. We do this with a custom R script which can be called as follows: Rscript plotting.R --rds {input} --runtype {run_type} --threads {threads} --outdir {outdir} Just like the preprocessing section, the copy number calling is also automatized and wrapped in a snakemake pipeline and is available in the github repository here. "],["technical-validation-of-sccutseq.html", "4 Technical validation of scCUTseq 4.1 Duplication rates of CUTseq vs MALBAC 4.2 Pearson correlation MALBAC scaling 4.3 Representative scCUTseq and sMALBAC profiles of SKBR3 cells 4.4 Lorenz curves of (sc)CUTseq and (scaled) MALBAC 4.5 Pearson correlation of fixed versus live SKBR3 cells 4.6 Genomewide copy number profiles of cell lines 4.7 Cross-contamination experiment with Dm and Hs 4.8 Detection of CRISPR induced deletions", " 4 Technical validation of scCUTseq This sections produces all the figures used in Supplementary Figure 1 and 2. # Source setup file source(&quot;./functions/setup.R&quot;) # Source plotting functions source(&quot;./functions/plotProfile.R&quot;) source(&quot;./functions/plotHeatmap.R&quot;) 4.1 Duplication rates of CUTseq vs MALBAC Duplication rate of CUTseq performed on a single cell and MALBAC with different volume scaling. dup_rate = readRDS(&quot;./data/technical_validation/CUTseq_MALBAC_duplication_rate.rds&quot;) counts = dup_rate[, .N, by = library] ggplot(dup_rate, aes(x = library, y = duplication)) + geom_violin(aes(fill = library, color = library)) + geom_boxplot(width = .075) + scale_fill_viridis_d(begin = .3) + scale_color_viridis_d(begin = .3) + scale_y_continuous(labels = scales::label_percent()) + geom_text(data = counts, aes(y = 0.1, x = library, label = paste0(&quot;n = &quot;, N))) + labs(y = &quot;Duplication rate&quot;, x = &quot;&quot;) + theme(legend.position = &quot;none&quot;) 4.2 Pearson correlation MALBAC scaling Pairwise Pearson correlation between scCUTseq on SKBR3 cells using different MALBAC scalings ref = readRDS(&quot;./data/technical_validation/cnv_SKBR_bulk_CUTseq.rds&quot;) mb50 = readRDS(&quot;./data/technical_validation/cnv_MALBAC_50.rds&quot;) mb100 = readRDS(&quot;./data/technical_validation/cnv_MALBAC_100.rds&quot;) mb200 = readRDS(&quot;./data/technical_validation/cnv_MALBAC_200.rds&quot;) mb500 = readRDS(&quot;./data/technical_validation/cnv_MALBAC_500.rds&quot;) # Select copynumbers ref_cn = ref$copynumber$TGATGCGC mb50_cn = mb50$copynumber mb100_cn = mb100$copynumber mb200_cn = mb200$copynumber mb500_cn = mb500$copynumber # Pairwise correlation mb50_pw = cor(mb50_cn) mb50_pw = data.table(sample = &quot;1:50&quot;, V1 = rownames(mb50_pw)[row(mb50_pw)[upper.tri(mb50_pw, diag = F)]], V2 = colnames(mb50_pw)[col(mb50_pw)[upper.tri(mb50_pw, diag = F)]], pearson = c(mb50_pw[upper.tri(mb50_pw, diag = F)])) mb100_pw = cor(mb100_cn) mb100_pw = data.table(sample = &quot;1:100&quot;, V1 = rownames(mb100_pw)[row(mb100_pw)[upper.tri(mb100_pw, diag = F)]], V2 = colnames(mb100_pw)[col(mb100_pw)[upper.tri(mb100_pw, diag = F)]], pearson = c(mb100_pw[upper.tri(mb100_pw, diag = F)])) mb200_pw = cor(mb200_cn) mb200_pw = data.table(sample = &quot;1:200&quot;, V1 = rownames(mb200_pw)[row(mb200_pw)[upper.tri(mb200_pw, diag = F)]], V2 = colnames(mb200_pw)[col(mb200_pw)[upper.tri(mb200_pw, diag = F)]], pearson = c(mb200_pw[upper.tri(mb200_pw, diag = F)])) mb500_pw = cor(mb500_cn) mb500_pw = data.table(sample = &quot;1:500&quot;, V1 = rownames(mb500_pw)[row(mb500_pw)[upper.tri(mb500_pw, diag = F)]], V2 = colnames(mb500_pw)[col(mb500_pw)[upper.tri(mb500_pw, diag = F)]], pearson = c(mb500_pw[upper.tri(mb500_pw, diag = F)])) res_pw = rbindlist(list(mb50_pw, mb100_pw, mb200_pw, mb500_pw)) # Prepare for plotting res_pw[, sample := factor(sample, levels = c(&quot;1:50&quot;, &quot;1:100&quot;, &quot;1:200&quot;, &quot;1:500&quot;))] obs = res_pw[, .N, by = sample] # Plot ggplot(res_pw, aes(x = sample, y = pearson)) + geom_violin(aes(fill = sample)) + geom_boxplot(width = .04, outlier.size = .5) + geom_text(data = obs, aes(y = 0, label = paste(&quot;n =&quot;, N))) + scale_fill_viridis_d(begin = .4) + scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .2), labels = seq(0, 1, .2)) + labs(y = &quot;Pairwise Pearson&#39;s Correlation&quot;, x = &quot;&quot;) + theme(legend.position = &quot;none&quot;) Pearson correlation between scCUTseq with different MALBAC scalings and bulk CUTseq # Correlation against ref bulk CUTseq mb50_cor = data.table(sample = &quot;1:50&quot;, pearson = as.vector(cor(mb50_cn, ref_cn))) mb100_cor = data.table(sample = &quot;1:100&quot;, pearson = as.vector(cor(mb100_cn, ref_cn))) mb200_cor = data.table(sample = &quot;1:200&quot;, pearson = as.vector(cor(mb200_cn, ref_cn))) mb500_cor = data.table(sample = &quot;1:500&quot;, pearson = as.vector(cor(mb500_cn, ref_cn))) res = rbindlist(list(mb50_cor, mb100_cor, mb200_cor, mb500_cor)) res[, sample := factor(sample, levels = c(&quot;1:50&quot;, &quot;1:100&quot;, &quot;1:200&quot;, &quot;1:500&quot;))] obs = res[, .N, by = sample] # Plot ggplot(res, aes(x = sample, y = pearson)) + geom_violin(aes(fill = sample)) + geom_boxplot(width = .04, outlier.size = .5) + geom_text(data = obs, aes(y = 0, label = paste(&quot;n =&quot;, N))) + scale_fill_viridis_d(begin = .4) + scale_y_continuous(limit = c(0, 1), breaks = seq(0, 1, .2), labels = seq(0, 1, .2)) + labs(y = &quot;Pearson&#39;s Correlation to bulk CUTseq&quot;, x = &quot;&quot;) + theme(legend.position = &quot;none&quot;) 4.3 Representative scCUTseq and sMALBAC profiles of SKBR3 cells smalbac_profile = readRDS(&quot;./data/technical_validation/smalbac_bc221.rds&quot;) scCUTseq_profile = readRDS(&quot;./data/technical_validation/scCUTseq_NZ40.rds&quot;) # Plot the profiles plotProfile(smalbac_profile$segments_read[[&quot;NZ58&quot;]], smalbac_profile$counts_gc[[&quot;NZ58&quot;]], smalbac_profile$bins) plotProfile(scCUTseq_profile$segments_read[[&quot;ACTGAGAT&quot;]], scCUTseq_profile$counts_gc[[&quot;ACTGAGAT&quot;]], scCUTseq_profile$bins) 4.4 Lorenz curves of (sc)CUTseq and (scaled) MALBAC Lorenz curves of Bulk CUTseq, MALBAC (fixed), MALBAC (live), sMALBAC (fixed), sMALBAC (live) and two fixed scCUTseq cells raw = readRDS(&quot;./data/technical_validation/lorenz-counts-500kb.rds&quot;) # Give names setnames(raw, c(&quot;NEBNext - live 1:1&quot;, &quot;NEBNext - fixed 1:1&quot;, &quot;NEBNext - live 1:200&quot;, &quot;NEBNext - fixed 1:200&quot;, &quot;scCUTseq - Cell 1&quot;, &quot;scCUTseq - Cell 2&quot;, &quot;Bulk CUTseq&quot;)) lorenz = lapply(colnames(raw), function(sample) { # Get lorenz curve points lc = Lc(raw[[sample]]) return(data.table(l = lc$L, p = lc$p, sample = sample)) }) lorenz = rbindlist(lorenz) ggplot(lorenz, aes(x = p, y = l, color = sample)) + geom_abline(slope = 1, size = 1.25) + geom_path(aes(group = sample), size = 1.25) + scale_y_continuous(expand = c(0, 0)) + scale_x_continuous(expand = c(0, 0)) + scale_color_npg() + labs(y = &quot;Cumulative fraction of total reads&quot;, x = &quot;Cumulative fraction of genome&quot;, color = &quot;&quot;) + coord_equal() 4.5 Pearson correlation of fixed versus live SKBR3 cells Pearson correlation of fixed and live SKBR3 libraries prepared with either scaled down (1:200) MALBAC or standard MALBAC followed by commercial library preparation. files = c(&quot;./data/technical_validation/smalbac_bc221.rds&quot;, &quot;./data/technical_validation/malbac_bc229.rds&quot;) total = lapply(files, function(i) { rds = readRDS(i) return(rds$copynumber) }) dt = do.call(cbind, total) setnames(dt, c(&quot;MALBAC 1:200 - fixed (cell 3)&quot;, &quot;MALBAC 1:200 - live (cell 1)&quot;, &quot;MALBAC 1:200 - fixed (cell 1)&quot;, &quot;MALBAC 1:200 - live (cell 2)&quot;, &quot;MALBAC 1:200 - live (cell 3)&quot;, &quot;MALBAC 1:200 - fixed (cell 2)&quot;, &quot;MALBAC 1:1 - live (cell 4)&quot;, &quot;MALBAC 1:1 - fixed (cell 4)&quot;, &quot;MALBAC 1:1 - fixed (cell 2)&quot;, &quot;MALBAC 1:1 - live (cell 1)&quot;, &quot;MALBAC 1:1 - fixed (cell 3)&quot;, &quot;MALBAC 1:1 - fixed (cell 1)&quot;, &quot;MALBAC 1:1 - live (cell 3)&quot;, &quot;MALBAC 1:1 - live (cell 2)&quot;)) # Plot correlations res = cor(dt) res_m = reshape2::melt(res, na.rm = T) setDT(res_m) res_m = res_m[grepl(&quot;live&quot;, Var1) &amp; grepl(&quot;fixed&quot;, Var2), ] res_m[, Var1 := factor(Var1, levels = c(&quot;MALBAC 1:200 - live (cell 1)&quot;, &quot;MALBAC 1:200 - live (cell 2)&quot;, &quot;MALBAC 1:200 - live (cell 3)&quot;, &quot;MALBAC 1:1 - live (cell 1)&quot;, &quot;MALBAC 1:1 - live (cell 2)&quot;, &quot;MALBAC 1:1 - live (cell 3)&quot;, &quot;MALBAC 1:1 - live (cell 4)&quot;))] res_m[, Var2 := factor(Var2, levels = rev(c(&quot;MALBAC 1:200 - fixed (cell 1)&quot;, &quot;MALBAC 1:200 - fixed (cell 2)&quot;, &quot;MALBAC 1:200 - fixed (cell 3)&quot;, &quot;MALBAC 1:1 - fixed (cell 1)&quot;, &quot;MALBAC 1:1 - fixed (cell 2)&quot;, &quot;MALBAC 1:1 - fixed (cell 3)&quot;, &quot;MALBAC 1:1 - fixed (cell 4)&quot;)))] ggplot(res_m, aes(x = Var1, y = Var2, fill = value)) + geom_tile() + scale_y_discrete(&quot;&quot;) + scale_x_discrete(&quot;&quot;, position = &quot;top&quot;) + geom_text(aes(label = round(value, 3))) + scale_fill_viridis(&quot;Pearson&#39;s\\ncorrelation&quot;, option=&quot;B&quot;, begin = 0.75, direction = -1) + theme(axis.text.x = element_text(angle = 45, hjust = 0, vjust = 0.5), axis.line = element_blank()) 4.6 Genomewide copy number profiles of cell lines Heatmap of genomewide copy number profiles of SKBR3 (live and fixed), IMR90 and MCF10A cell lines # Read in cnv.rds and extract HQ profiles cn = readRDS(&quot;./data/technical_validation/cell_lines_heatmap.rds&quot;) # Get annotation annot = data.table(samples = factor(colnames(cn[, 4:ncol(cn)])), variable = &quot;cell_type&quot;, value = gsub(&quot;-.*&quot;, &quot;&quot;, colnames(cn[, 4:ncol(cn)]))) # Plot heatmap plotHeatmap(cn[, 4:ncol(cn)], cn[, 1:3], annotation = annot, linesize = 2) 4.7 Cross-contamination experiment with Dm and Hs # Load in total number of reads (pre and post deduplication) counts = readRDS(&quot;./data/technical_validation/hs_dm_counts.rds&quot;) # Melt datatable counts = melt(counts) # Format for plotting counts[, value := value / 1e6] counts[, cell := gsub(&quot; POS&quot;, &quot;&quot;, cell)] counts[, replicate := gsub(&quot;.*_&quot;, &quot;&quot;, variable)] counts[, variable := gsub(&quot;_rep.*&quot;, &quot;&quot;, variable)] counts[, variable := factor(variable, levels = c(&quot;all_hs&quot;, &quot;all_dm&quot;))] ggplot(counts, aes(x=variable, y=value, color = cell)) + geom_boxplot(outlier.shape = NA) + geom_point(position = position_jitterdodge(jitter.width = .2)) + facet_wrap(~replicate) + labs(y = &quot;Reads per cell (M)&quot;, x = &quot;&quot;, color = &quot;&quot;) + scale_color_manual(values = brewer.pal(4, &quot;Set1&quot;)) + theme(legend.position = &quot;top&quot;) 4.8 Detection of CRISPR induced deletions Select cells that have some type of alteration in the CRISPR area and then plot a zoom-in heatmap of this region. # Load TK6 data treated = readRDS(&quot;./data/TK6_treated.rds&quot;) untreated = readRDS(&quot;./data/TK6_untreated.rds&quot;) # Set bins bins = treated[, 1:3] # Set region of interest for heatmap roi = data.table(chr = &quot;11&quot;, start = 118307205, end = 125770541) # This is the CRISPR targeted region roi_bins = foverlaps(roi, bins, which = T)$yid # Select cells that have any part of the ROI altered (cells of interest; COI) treated_coi = unlist(sapply(colnames(treated[, 4:ncol(treated)]), function(x) { count = sum(treated[roi_bins, ..x] != 2) if(count &gt; 2) return(x) })) untreated_coi = unlist(sapply(colnames(untreated[, 4:ncol(untreated)]), function(x) { count = sum(untreated[roi_bins, ..x] != 2) if(count &gt; 2) return(x) })) # Get bins of -10mb and +10 mb of gRNAs closest = data.table(chr = &quot;11&quot;, start = c(roi$start[1] - 1e7, roi$end[1] + 1e7)) setkey(closest, chr, start) plot_window = bins[closest, roll = &quot;nearest&quot;] zoom_index = which(bins$chr == plot_window[1, chr] &amp; bins$start &gt;= plot_window[1, start] &amp; bins$end &lt;= plot_window[2, end]) # Subset data on both samples and genomic region total = cbind(treated[zoom_index, ..treated_coi], untreated[zoom_index, ..untreated_coi]) bins = bins[zoom_index] # Make annotation data table annot = data.table(sample = c(treated_coi, untreated_coi), variable = &quot;condition&quot;, value = c(rep(&quot;treated&quot;, length(treated_coi)), rep(&quot;untreated&quot;, length(untreated_coi)))) # Plot heatmap plotHeatmap(total, bins, annotation = annot, linesize = 2) Plot UMAPs of chromosome 11 copy number profiles. Take note that we did not set a seed for this analysis, so the UMAP can look slightly different than the figure we show. The differences are minimal and do not change any of the conclusions we draw from these plots. dt = merge(treated, untreated, by = c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;)) # Make UMAP dataframe only using chr 11 profiles total_umap = umap(t(dt[chr == &quot;11&quot;, 4:ncol(dt)]), n_neighbors = 24, spread = 1, min_dist = 0) umap_dt = data.table(x = total_umap[, 1], y = total_umap[, 2], sample = colnames(dt[, 4:ncol(dt)]), group = ifelse(colnames(dt[, 4:ncol(dt)]) %in% colnames(treated), &quot;Treated&quot;, &quot;Untreated&quot;)) # Plot UMAP ggplot(umap_dt, aes(x = x, y = y, color = group)) + geom_point(size = 1.5) + scale_color_npg() + labs(x = &quot;UMAP 1&quot;, y = &quot;UMAP 2&quot;, color = &quot;&quot;) # Cluster UMAP using DBSCAN clones = dbscan::hdbscan(umap_dt[,c(1:2)], minPts = 9) umap_dt[, clone := LETTERS[clones$cluster + 1]] # Plot clusters ggplot(umap_dt, aes(x = x, y = y, color = clone)) + geom_point(size = 2) + scale_color_npg() + labs(x = &quot;UMAP 1&quot;, y = &quot;UMAP 2&quot;, color = &quot;&quot;) "],["comparison-of-sccutseq-and-act.html", "5 Comparison of scCUTseq and ACT 5.1 Calculate breadth of coverage 5.2 Calculate overdispersion 5.3 Plot genomewide heatmaps of ACT and scCUTseq copynumber profiles 5.4 Plot profile plots of example profiles", " 5 Comparison of scCUTseq and ACT This sections produces all the figures used in Supplementary Figure 3. We compare the Breadth of Coverage, Overdispersion and overall copynumber profiles between different sections in Patient 3 that were sequenced by both ACT and scCUTseq. # Source setup file source(&quot;./functions/setup.R&quot;) # Source plotting functions source(&quot;./functions/plotProfile.R&quot;) source(&quot;./functions/plotHeatmap.R&quot;) 5.1 Calculate breadth of coverage We downsampled bam files to 800K reads using the following commands in bash and ran our copynumber calling pipeline (described in Preprocessing.Rmd) and use the resulting cnv.rds to get information about cell quality. samtools view ${INSAMPLEBASE}.bam -H &gt; ${INSAMPLEBASE}.sam&amp;&amp; samtools view $INSAMPLEBASE.bam | shuf -n 800000 --random-source=$INSAMPLEBASE.bam &gt;&gt; $INSAMPLEBASE.sam&amp;&amp; samtools sort -o ../downsampling/bamfiles/$OUTBAM $INSAMPLEBASE.sam&amp;&amp; rm $INSAMPLEBASE.sam&amp;&amp; genomeCoverageBed -ibam ../downsampling/bamfiles/$OUTBAM -fs 50 -max 1 &gt; ../downsampling/coverage/$COVHISTFILE We then used the $COVHISTFILE output to calculate the breadth of coverage. # Define BoC function calc_coverage = function(path) { inpaths = Sys.glob(paste0(path, &quot;*.covhist.txt&quot;)) coverage.stats = tibble(bed_path=inpaths) %&gt;% mutate(cellname = str_extract(basename(bed_path), &quot;^[^.]*&quot;)) %&gt;% group_by(cellname) %&gt;% summarize(.groups=&quot;keep&quot;, read_tsv(bed_path, col_names=c(&quot;refname&quot;, &quot;depth&quot;, &quot;count&quot;, &quot;refsize&quot;, &quot;frac&quot;), col_types=cols(col_character(), col_double(), col_double(), col_double(), col_double())), ) %&gt;% filter(refname==&quot;genome&quot;) %&gt;% summarize(breadth = 1 - frac[depth==0], .groups=&quot;keep&quot;) } # Calculate BoC on the COVHIST files sccutseq = calc_coverage(&quot;./data/downsampling/coverage/scCUTseq/&quot;) act = calc_coverage(&quot;./data/downsampling/coverage/ACT/&quot;) # SetDT setDT(sccutseq) setDT(act) # Load copynumber profiles to get HQ cells p3 = readRDS(&quot;./data/downsampling/P3.rds&quot;) p3_stats = p3$stats p3_hq = p3_stats[classifier_prediction == &quot;good&quot;, sample] # ACT CD5p = readRDS(&quot;./data/downsampling/CD5p.rds&quot;) CD7p = readRDS(&quot;./data/downsampling/CD7p.rds&quot;) CD4p = readRDS(&quot;./data/downsampling/CD4p.rds&quot;) CD2p = readRDS(&quot;./data/downsampling/CD2p.rds&quot;) CD6p = readRDS(&quot;./data/downsampling/CD6p.rds&quot;) CD3p = readRDS(&quot;./data/downsampling/CD3p.rds&quot;) act_hq = c(paste0(&quot;CD5p_&quot;, CD5p$stats[classifier_prediction == &quot;good&quot;, sample]), paste0(&quot;CD7p_&quot;, CD7p$stats[classifier_prediction == &quot;good&quot;, sample]), paste0(&quot;CD4p_&quot;, CD4p$stats[classifier_prediction == &quot;good&quot;, sample]), paste0(&quot;CD2p_&quot;, CD2p$stats[classifier_prediction == &quot;good&quot;, sample]), paste0(&quot;CD6p_&quot;, CD6p$stats[classifier_prediction == &quot;good&quot;, sample]), paste0(&quot;CD3p_&quot;, CD3p$stats[classifier_prediction == &quot;good&quot;, sample])) # Select HQ samples sccutseq = sccutseq[cellname %in% p3_hq, ] act = act[cellname %in% act_hq, ] # Combine sccutseq[, tech := &quot;scCUTseq&quot;] act[, tech := &quot;ACT&quot;] dt = rbind(sccutseq, act) dt[, tech := factor(tech, levels = c(&quot;scCUTseq&quot;, &quot;ACT&quot;))] # Plot comparison ggplot(dt, aes(x = tech, y = breadth)) + geom_violin() + geom_boxplot(width = .15) + scale_y_continuous(limits = c(0, 0.0125)) + labs(y = &quot;Breadth of coverage&quot;, x = &quot;&quot;) 5.2 Calculate overdispersion Then we used the read counts from each bin to calculate the overdispersion. We also do this on the HQ cells that are selected in the code chunk above. # Define functions l2e.normal.sd = function(xs) { # Need at least two values to get a standard deviation stopifnot(length(xs) &gt;= 2) optim.result = stats::optimize( # L2E loss function f=function(sd) # &quot;Data part&quot;, the sample average of the likelihood -2 * mean(stats::dnorm(xs, sd=sd)) + # &quot;Theta part&quot;, the integral of the squared density 1/(2*sqrt(pi)*sd), # Parameter: standard deviation of the normal distribution fit interval = c(0, diff(range(xs)))) return(optim.result$minimum) } # A function for estimating the index of dispersion, which is used when # estimating standard errors for each segment mean overdispersion = function(v) { # 3 elements, 2 differences, can find a standard deviation stopifnot(length(v) &gt;= 3) # Differences between pairs of values y = v[-1] x = v[-length(v)] # Normalize the differences using the sum. The result should be around zero, # plus or minus square root of the index of dispersion vals.unfiltered = (y-x)/sqrt(y+x) # Remove divide by zero cases, and--considering this is supposed to be count # data--divide by almost-zero cases vals = vals.unfiltered[y + x &gt;= 1] # Check that there&#39;s anything left stopifnot(length(vals) &gt;= 2) # Assuming most of the normalized differences follow a normal distribution, # estimate the standard deviation val.sd = l2e.normal.sd(vals) # Square this standard deviation to obtain an estimate of the index of # dispersion iod = val.sd^2 # subtract one to get the overdispersion criteria iod.over = iod -1 # normalizing by mean bincounts iod.norm = iod.over/mean(v) return(iod.norm) } # Get count data for scCUTseq and ACT p3_counts = p3$counts[, p3_stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD5p_counts = CD5p$counts[, CD5p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD7p_counts = CD7p$counts[, CD7p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD4p_counts = CD4p$counts[, CD4p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD2p_counts = CD2p$counts[, CD2p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD6p_counts = CD6p$counts[, CD6p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] CD3p_counts = CD3p$counts[, CD3p$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] # Combine act act_counts = do.call(cbind, list(CD5p_counts, CD4p_counts, CD2p_counts, CD7p_counts, CD6p_counts, CD3p_counts)) # Calculate overdispersion p3_overdispersion = lapply(p3_counts, function(x) { overdispersion(x) }) act_overdispersion = lapply(act_counts, function(x) { overdispersion(x) }) # Make into DT act_dt = data.table(method = &quot;ACT&quot;, overdispersion = unlist(act_overdispersion)) p3_dt = data.table(method = &quot;scCUTseq&quot;, overdispersion = unlist(p3_overdispersion)) # Combine DTs dt = rbind(act_dt, p3_dt) dt[, method := factor(method, levels = c(&quot;scCUTseq&quot;, &quot;ACT&quot;))] # Plot comparison ggplot(dt, aes(x = method, y = overdispersion)) + geom_violin() + geom_boxplot(width = .075) + labs(y = &quot;Overdispersion&quot;, x = &quot;&quot;) 5.3 Plot genomewide heatmaps of ACT and scCUTseq copynumber profiles Plot both the ACT and scCUTseq genomewide heatmap to visually compare the two methods. Note: the plotting of these can take a while. Furthermore, we set dendrogram = FALSE since there is a known bug with the ggdendro package when you have many samples that are identical (in our case, diploid cells) # Load in data p3_sccut = readRDS(&quot;./data/P3_cnv.rds&quot;) # Select HQ cells p3_sccut_profiles = p3_sccut$copynumber[, p3_sccut$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] p3_sccut_profiles = p3_sccut_profiles[, grepl(&quot;NZ186|MS80|NZ187|NZ188|NZ189|NZ211&quot;, colnames(p3_sccut_profiles)), with = F] # Select sections that also has ACT sequencing data # Make annotation table for scCUTseq p3_annot = fread(&quot;./annotation/P3.tsv&quot;, header = FALSE) scCUT_annot = data.table(sample = colnames(p3_sccut_profiles), variable = &quot;section&quot;, value = gsub(&quot;_.*&quot;, &quot;&quot;, colnames(p3_sccut_profiles))) # Merge to get section information scCUT_annot = merge(scCUT_annot, p3_annot, by.x = &quot;value&quot;, by.y = &quot;V1&quot;) # Plot scCUTseq heatmap plotHeatmap(p3_sccut_profiles, p3_sccut$bins, dendrogram = FALSE, annotation = scCUT_annot[, .(sample, variable, V2)]) # Get ACT profiles and make annotation p3_act = readRDS(&quot;./data/P3_ACT.rds&quot;) p3_act_profiles = p3_act$copynumber[, sort(p3_act$stats[classifier_prediction == &quot;good&quot;, sample]), with = F] # Make annotation for ACT ACT_annot = data.table(sample = colnames(p3_act_profiles), variable = &quot;section&quot;, value = gsub(&quot;_.*&quot;, &quot;&quot;, colnames(p3_act_profiles))) # Plot ACT heatmap plotHeatmap(p3_act_profiles, p3_act$bins, dendrogram = FALSE, annotation = ACT_annot) 5.4 Plot profile plots of example profiles We selected 2 profiles of scCUTseq and 2 profiles of ACT to show the differences. These are selected manually but are representative for other plots (you can change the sample ID to manually check others) # profile selection sccut = readRDS(&quot;./data/CD27.rds&quot;) act = readRDS(&quot;./data/CD1p.rds&quot;) # scCUTseq cell 1 plotProfile(sccut$copynumber$AGCCAACGGCA, sccut$counts_gc$AGCCAACGGCA * sccut$ploidies[sample == &quot;AGCCAACGGCA&quot;, ploidy], sccut$bins) # scCUTseq cell 2 plotProfile(sccut$copynumber$AGCTTGCTCAT, sccut$counts_gc$AGCTTGCTCAT * sccut$ploidies[sample == &quot;AGCTTGCTCAT&quot;, ploidy], sccut$bins) # ACT cell 1 plotProfile(act$copynumber$`296_S296`, act$counts_gc$`296_S296` * act$ploidies[sample == &quot;296_S296&quot;, ploidy], act$bins) # ACT cell 2 plotProfile(act$copynumber$`227_S227`, act$counts_gc$`227_S227` * act$ploidies[sample == &quot;227_S227&quot;, ploidy], act$bins) "],["spatially-resolved-single-cell-cna-profiling-across-the-prostate.html", "6 Spatially resolved single-cell CNA profiling across the prostate 6.1 Spatial pathologist annotation 6.2 Spatial distribution of RNAseq clusters, SCNAs and mutations in prostate 6.3 Cell type distribution", " 6 Spatially resolved single-cell CNA profiling across the prostate This section produces all the figures used for Figure 1. # Source setup file source(&quot;./functions/setup.R&quot;) 6.1 Spatial pathologist annotation Load in pathologist annotation data p3_annot = fread(&quot;./annotation/P3.tsv&quot;, header = FALSE, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;pathology_annotation&quot;)) p6_annot = fread(&quot;./annotation/P6.tsv&quot;, header = FALSE, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;pathology_annotation&quot;)) Plot the annotation data in the schematic of the prostate of both patients. # Extract x and y coordinates p3_annot[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] p3_annot[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] p6_annot[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] p6_annot[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Plot ggplot(p3_annot, aes(x = x, y = y, fill = pathology_annotation)) + geom_tile() + geom_hline(yintercept = seq(from = .5, to = max(p3_annot$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p3_annot$x), by = 1)) + scale_fill_npg() + labs(title = &quot;P3 Pathology annotation&quot;) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(p3_annot$y)), labels = seq(1, max(p3_annot$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(p3_annot$x)), labels = seq(1, max(p3_annot$x))) + theme(axis.title = element_blank()) ggplot(p6_annot, aes(x = x, y = y, fill = pathology_annotation)) + geom_tile() + geom_hline(yintercept = seq(from = .5, to = max(p6_annot$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p6_annot$x), by = 1)) + scale_fill_npg() + labs(title = &quot;P6 Pathology annotation&quot;) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(p6_annot$y)), labels = seq(1, max(p6_annot$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(p6_annot$x)), labels = seq(1, max(p6_annot$x))) + theme(axis.title = element_blank()) 6.2 Spatial distribution of RNAseq clusters, SCNAs and mutations in prostate Load in RNAseq clusters, copynumber profiles and mutations. In the case of % of cells with SCNAs, we take all the completely diploid cells and take 1 - (fraction of diploid) to get cells with SCNAs. # Patient 3 p3_cnv = readRDS(&quot;data/P3_cnv.rds&quot;) p3_rnaseq = fread(&quot;data/P3_RNAseq_clusters.tsv&quot;) p3_mutations = fread(&quot;data/P3_nMuts.tsv&quot;) # Patient 6 p6_cnv = readRDS(&quot;data/P6_cnv.rds&quot;) p6_rnaseq = fread(&quot;data/P6_RNAseq_clusters.tsv&quot;) p6_mutations = fread(&quot;data/P6_nMuts.tsv&quot;) Classifiy copynumber profiles into the three cell groups based on the percentage of the genome that is altered and plot the percentages. profiles = p3_cnv$copynumber[, p3_cnv$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] bins = p3_cnv$bins # Remove all fully diploid cells diploid_vector = c(rep(2, sum(bins$chr != &quot;X&quot;)), rep(1, sum(bins$chr == &quot;X&quot;))) # Keep diploid_check = sapply(colnames(profiles), function(cell) { sum(profiles[[cell]] != diploid_vector) }) dt = data.table(sample = names(diploid_check), num_altered = diploid_check) # Divide based on number of alterations diploid_p3 = profiles[, dt[num_altered == 0, sample], with = FALSE] pseudodiploid_p3 = profiles[, dt[num_altered != 0 &amp; num_altered &lt;= .25 * nrow(bins), sample], with = FALSE] monster_p3 = profiles[, dt[num_altered != 0 &amp; num_altered &gt; .25 * nrow(bins), sample], with = FALSE] # Make into DT for plotting overall_p3 = data.table(cell_type = c(&quot;diploid&quot;, &quot;pseudodiploid&quot;, &quot;monster&quot;), number = c(ncol(diploid_p3), ncol(pseudodiploid_p3), ncol(monster_p3)), patient = &quot;P3&quot;) overall_p3[, percent := number / sum(number) * 100] profiles = p6_cnv$copynumber[, p6_cnv$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] bins = p6_cnv$bins # Remove all fully diploid cells diploid_vector = c(rep(2, sum(bins$chr != &quot;X&quot;)), rep(1, sum(bins$chr == &quot;X&quot;))) # Keep diploid_check = sapply(colnames(profiles), function(cell) { sum(profiles[[cell]] != diploid_vector) }) dt = data.table(sample = names(diploid_check), num_altered = diploid_check) # Divide based on number of alterations diploid_p6 = profiles[, dt[num_altered == 0, sample], with = FALSE] pseudodiploid_p6 = profiles[, dt[num_altered != 0 &amp; num_altered &lt;= .25 * nrow(bins), sample], with = FALSE] monster_p6 = profiles[, dt[num_altered != 0 &amp; num_altered &gt; .25 * nrow(bins), sample], with = FALSE] # Make into DT for plotting overall_p6 = data.table(cell_type = c(&quot;diploid&quot;, &quot;pseudodiploid&quot;, &quot;monster&quot;), number = c(ncol(diploid_p6), ncol(pseudodiploid_p6), ncol(monster_p6)), patient = &quot;P6&quot;) overall_p6[, percent := number / sum(number) * 100] Prepare the data for plotting. RNAseq and mutations are ready for plotting (just add coordinates) but we still need to extract the percentage of cells that have SCNAs from the copynumber profiles. We use the previously compiled list of diploid cells for this. # Make data.table with cell names and type # P3 dt_p3 = data.table(sample = c(colnames(diploid_p3), colnames(pseudodiploid_p3), colnames(monster_p3)), celltype = c(rep(&quot;diploid&quot;, ncol(diploid_p3)), rep(&quot;pseudodiploid&quot;, ncol(pseudodiploid_p3)), rep(&quot;monster&quot;, ncol(monster_p3)))) dt_p3[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] # P6 dt_p6 = data.table(sample = c(colnames(diploid_p6), colnames(pseudodiploid_p6), colnames(monster_p6)), celltype = c(rep(&quot;diploid&quot;, ncol(diploid_p6)), rep(&quot;pseudodiploid&quot;, ncol(pseudodiploid_p6)), rep(&quot;monster&quot;, ncol(monster_p6)))) dt_p6[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] # Get counts per section total_p3 = dt_p3[, .N, by = .(library)] total_p6 = dt_p6[, .N, by = .(library)] # Get diploid counts per section count_p3 = dt_p3[celltype == &quot;diploid&quot;, .N, by = .(library)] count_p6 = dt_p6[celltype == &quot;diploid&quot;, .N, by = .(library)] # Merge and get fraction of non-diploid (1-diploid) count_p3 = merge(count_p3, total_p3, by = &quot;library&quot;) count_p6 = merge(count_p6, total_p6, by = &quot;library&quot;) count_p3[, nondiploid_fraction := 1 - (N.x / N.y)] count_p6[, nondiploid_fraction := 1 - (N.x / N.y)] # Merge with annotation count_p3 = merge(count_p3, p3_annot) count_p6 = merge(count_p6, p6_annot) # Get coordinates count_p3[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] count_p3[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] count_p6[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] count_p6[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] Plot all the data for Patient 3 # Plot RNAseq clusters p3_rnaseq[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, V1))] p3_rnaseq[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, V1))] p3_rnaseq[, V3 := factor(V3, levels = c(&quot;2&quot;, &quot;1&quot;, &quot;3&quot;))] ggplot(p3_rnaseq, aes(x = x, y = y, fill = V3)) + geom_tile() + scale_fill_npg() + geom_hline(yintercept = seq(from = .5, to = max(p3_rnaseq$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p3_rnaseq$x), by = 1)) + labs(title = &quot;RNA-seq clusters (P3)&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(p3_rnaseq$y)), labels = seq(1, max(p3_rnaseq$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(p3_rnaseq$x)), labels = seq(1, max(p3_rnaseq$x))) + theme(axis.title = element_blank()) # Plot SCNAs ggplot(count_p3, aes(x = x, y = y, fill = nondiploid_fraction)) + geom_tile() + scale_fill_distiller(name = &quot;Percentage of cells\\nwith SCNAs&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, label = scales::percent_format()) + geom_hline(yintercept = seq(from = .5, to = max(count_p3$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(count_p3$x), by = 1)) + labs(title = &quot;Percentage of cells with SCNAs (P3)&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(count_p3$y)), labels = seq(1, max(count_p3$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(count_p3$x)), labels = seq(1, max(count_p3$x))) + theme(axis.title = element_blank()) # Plot number of mutations p3_mutations[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] p3_mutations[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] ggplot(p3_mutations, aes(x = x, y = y, fill = n_muts)) + geom_tile() + scale_fill_distiller(name = &quot;Percentage of cells\\nwith SCNAs&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limit = c(0, 12)) + geom_hline(yintercept = seq(from = .5, to = max(p3_mutations$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p3_mutations$x), by = 1)) + labs(title = &quot;Number of mutations per section (P3)&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(p3_mutations$y)), labels = seq(1, max(p3_mutations$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(p3_mutations$x)), labels = seq(1, max(p3_mutations$x))) + theme(axis.title = element_blank()) Plot all the data for Patient 6 # Plot RNAseq clusters p6_rnaseq[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, V1))] p6_rnaseq[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, V1))] p6_rnaseq[, V3 := factor(V3, levels = c(&quot;2&quot;, &quot;1&quot;, &quot;3&quot;))] ggplot(p6_rnaseq, aes(x = x, y = y, fill = V3)) + geom_tile() + scale_fill_npg() + geom_hline(yintercept = seq(from = .5, to = max(p6_rnaseq$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p6_rnaseq$x), by = 1)) + labs(title = &quot;RNA-seq clusters (P6)&quot;, fill = &quot;RNA-seq cluster&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(p6_rnaseq$y)), labels = seq(1, max(p6_rnaseq$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(p6_rnaseq$x)), labels = seq(1, max(p6_rnaseq$x))) + theme(axis.title = element_blank()) # Plot SCNAs ggplot(count_p6, aes(x = x, y = y, fill = nondiploid_fraction)) + geom_tile() + scale_fill_distiller(name = &quot;Percentage of cells\\nwith SCNAs&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, label = scales::percent_format()) + geom_hline(yintercept = seq(from = .5, to = max(count_p6$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(count_p6$x), by = 1)) + labs(title = &quot;Percentage of cells with SCNAs (P6)&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(count_p6$y)), labels = seq(1, max(count_p6$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(count_p6$x)), labels = seq(1, max(count_p6$x))) + theme(axis.title = element_blank()) # Plot number of mutations p6_mutations[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] p6_mutations[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] ggplot(p6_mutations, aes(x = x, y = y, fill = n_muts)) + geom_tile() + scale_fill_distiller(name = &quot;Number of mutations&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limit = c(0, 12)) + geom_hline(yintercept = seq(from = .5, to = max(p6_mutations$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(p6_mutations$x), by = 1)) + labs(title = &quot;Number of mutations per section (P6)&quot;) + scale_y_reverse(expand = c(0, 0 ), breaks = seq(1, max(p6_mutations$y)), labels = seq(1, max(p6_mutations$y))) + scale_x_reverse(expand = c(0, 0 ), breaks = seq(1, max(p6_mutations$x)), labels = seq(1, max(p6_mutations$x))) + theme(axis.title = element_blank()) 6.3 Cell type distribution We detected three different major groups of cells, namely, Diploid cells, pseudo-diploid cells, and monster cells. Above, we already assigned our cells to the different celltype so we can now use this to plot the percentages. # Combine patients overall = rbind(overall_p3, overall_p6) # Reorder factors overall[, cell_type := factor(cell_type, levels = c(&quot;diploid&quot;, &quot;pseudodiploid&quot;, &quot;monster&quot;))] # Plot ggplot(overall, aes(x = patient, y = percent, fill = cell_type, label = number)) + geom_col() + scale_y_continuous(expand = c(0, 0)) + labs(y = &quot;Percentage of cell type&quot;, x = &quot;&quot;, fill = &quot;Cell type&quot;) + geom_text(aes(label = paste0(round(percent, 1), &quot;%\\n(n = &quot;, number, &quot;)&quot;)), position = position_stack(vjust = .5), size = 4) + scale_fill_npg() + theme(axis.ticks.x = element_blank()) Second, we show representative single-cell profiles for these celltypes. # Load in plotProfile function to plot copy number profiles source(&quot;functions/plotProfile.R&quot;) # Select random diploid, pseudo-diploid and monster cell for each patient diploid_p3_example = colnames(diploid_p3)[sample(1:ncol(diploid_p3), 1)] pseudodiploid_p3_example = colnames(pseudodiploid_p3)[sample(1:ncol(pseudodiploid_p3), 1)] monster_p3_example = colnames(monster_p3)[sample(1:ncol(monster_p3), 1)] diploid_p6_example = colnames(diploid_p6)[sample(1:ncol(diploid_p6), 1)] pseudodiploid_p6_example = colnames(pseudodiploid_p6)[sample(1:ncol(pseudodiploid_p6), 1)] monster_p6_example = colnames(monster_p6)[sample(1:ncol(monster_p6), 1)] # Plot representative profiles for diploid, pseudo-diploid and monster cells # P3 plotProfile(p3_cnv$copynumber[[diploid_p3_example]], p3_cnv$counts_gc[[diploid_p3_example]] * p3_cnv$ploidies[sample == diploid_p3_example, ploidy], bins = p3_cnv$bins) plotProfile(p3_cnv$copynumber[[pseudodiploid_p3_example]], p3_cnv$counts_gc[[pseudodiploid_p3_example]] * p3_cnv$ploidies[sample == pseudodiploid_p3_example, ploidy], bins = p3_cnv$bins) plotProfile(p3_cnv$copynumber[[monster_p3_example]], p3_cnv$counts_gc[[monster_p3_example]] * p3_cnv$ploidies[sample == monster_p3_example, ploidy], bins = p3_cnv$bins) # P6 plotProfile(p6_cnv$copynumber[[diploid_p6_example]], p6_cnv$counts_gc[[diploid_p6_example]] * p6_cnv$ploidies[sample == diploid_p6_example, ploidy], bins = p6_cnv$bins) plotProfile(p6_cnv$copynumber[[pseudodiploid_p6_example]], p6_cnv$counts_gc[[pseudodiploid_p6_example]] * p6_cnv$ploidies[sample == pseudodiploid_p6_example, ploidy], bins = p6_cnv$bins) plotProfile(p6_cnv$copynumber[[monster_p6_example]], p6_cnv$counts_gc[[monster_p6_example]] * p6_cnv$ploidies[sample == monster_p6_example, ploidy], bins = p6_cnv$bins) Plot distribution of celltypes in the different regions # Get counts counts_p3 = dt_p3[, .N, by = .(celltype, library)] counts_p6 = dt_p6[, .N, by = .(celltype, library)] # Merge with annotation counts_p3 = merge(counts_p3, p3_annot) counts_p6 = merge(counts_p6, p6_annot) # Merge with total counts per section (calculated in previous section) counts_p3 = merge(counts_p3, total_p3) counts_p6 = merge(counts_p6, total_p6) # Calculate fraction counts_p3[, fraction := N.x / N.y] counts_p6[, fraction := N.x / N.y] # Reorder factors counts_p3[, celltype := factor(celltype, levels = c(&quot;diploid&quot;, &quot;pseudodiploid&quot;, &quot;monster&quot;))] counts_p6[, celltype := factor(celltype, levels = c(&quot;diploid&quot;, &quot;pseudodiploid&quot;, &quot;monster&quot;))] # Plot P3 ggplot(counts_p3, aes(x = pathology_annotation, y = fraction, color = pathology_annotation)) + facet_wrap(~celltype) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = .25, size = 2) + scale_color_npg() + scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, .25)) + labs(y = &quot;Fraction of total cells&quot;, x = &quot;&quot;) + stat_compare_means(comparisons = list(c(&quot;Cancer&quot;, &quot;Focal&quot;), c(&quot;Focal&quot;, &quot;Normal&quot;), c(&quot;Cancer&quot;, &quot;Normal&quot;))) + theme(legend.position = &quot;&quot;) # Plot P6 ggplot(counts_p6, aes(x = pathology_annotation, y = fraction, color = pathology_annotation)) + facet_wrap(~celltype) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = .25, size = 2) + scale_color_npg() + scale_y_continuous(labels = scales::percent_format(), breaks = seq(0, 1, .25)) + labs(y = &quot;Fraction of total cells&quot;, x = &quot;&quot;) + stat_compare_means(comparisons = list(c(&quot;Cancer&quot;, &quot;Focal&quot;), c(&quot;Focal&quot;, &quot;Normal&quot;), c(&quot;Cancer&quot;, &quot;Normal&quot;))) + theme(legend.position = &quot;&quot;) "],["random-forest-classification.html", "7 Random forest classification 7.1 Loading in data 7.2 Obtain training set and validation set 7.3 Training the model 7.4 Validating the model 7.5 Plotting number of cells that pass QC", " 7 Random forest classification This sections produces all the figures used in Supplementary Figure 5. # Source setup file source(&quot;./functions/setup.R&quot;) 7.1 Loading in data After annotating the (2000ish) profiles using the shiny app from the previous section, we now load in all the metrics and remove sample/library names to not include that information in the training. Do note, because we recreate the RF model here, the results might differ slightly from the results and plots shown in the manuscript. # Load in metrics and user classification data metrics = fread(&quot;./data/RandomForest/metrics.csv&quot;) # Set factor and remove sample and library information since we do not want to include that in the training metrics[, user_quality := factor(user_quality, levels = c(&quot;good&quot;, &quot;bad&quot;))] metrics = metrics[!is.na(user_quality), !c(&quot;sample&quot;, &quot;library&quot;)] 7.2 Obtain training set and validation set We used 80% of our data to train our model on and then evaluated the RF model on the other 20% of the samples that the RF model has not encountered during training. It is important to check that the distribution of good/bad profiles is relatively equal in training and validation set and is a reflection of the full data. If this is not the case you can rerun the sampling (you can also set a seed to ensure you obtain the same sampling results) # Set size (in fraction of total) of the training set size_training = 0.8 # Randomly sample the training set and put the remaining samples in the validation set training_set = metrics[sample(.N, round(nrow(metrics) * size_training))] validation_set = fsetdiff(metrics, training_set) # Verify that the training and validation set have a good distribution of good/bad quality profiles (based on user annotation) table(metrics$user_quality)[1] / (table(metrics$user_quality)[1] + table(metrics$user_quality)[2]) # ratio of good vs bad ## good ## 0.3793403 table(training_set$user_quality)[1] / (table(training_set$user_quality)[1] + table(training_set$user_quality)[2]) # ratio of good vs bad ## good ## 0.3776451 table(validation_set$user_quality)[1] / (table(validation_set$user_quality)[1] + table(validation_set$user_quality)[2]) # ratio of good vs bad ## good ## 0.3861171 7.3 Training the model We trained the RF model with default settings since we obtained a high classification accuracy # Train model rf = randomForest(user_quality ~ ., data = training_set, importance = T) 7.4 Validating the model Following the training we need to validate the models performance using the validation set. First we run the model on the validation set using predict and add columns with information about the predicted quality and the actual quality. Following this we plot the Receiver operating characteristic (ROC) curve and check the Area Under the Curve (AUC). Depending on randomness (or which seed is set) the curve and AUC can vary slightly. But in our experience it has always been &gt;0.98 # Predict on validation set prediction = as.data.table(predict(rf, validation_set, type=&quot;prob&quot;)) prediction[, response := ifelse(good &gt; bad, &quot;good&quot;, &quot;bad&quot;)] prediction[, observation := validation_set$user_quality] # Plot receiver operator characteristics curve roc_curve = roc(prediction$observation, prediction$good) ggroc(roc_curve, size = 1.2) + annotate(&quot;text&quot;, y = 0.1, x = 0.1, label = paste0(&quot;AUC = &quot;, round(roc_curve$auc[[1]], 3))) Next, we wanted to see which variables (sequencing/copy number calling metrics) are the most important for the RF classification. # Get variable iomportance values var_imps = data.frame(rf$importance) var_imps$feature = rownames(var_imps) setDT(var_imps) setorder(var_imps, MeanDecreaseAccuracy) var_imps[, feature := factor(feature, levels = feature)] # Plot variable importances ggplot(var_imps, aes(x = MeanDecreaseAccuracy, y = feature)) + geom_segment(aes(xend = 0, yend = feature), size = 1.2) + geom_point(size = 4, color = &quot;orange&quot;) + labs(y = &quot;Feature&quot;, x = &quot;Mean decrease in accuracy&quot;) Finally, we calculate other measures such as the F1-score, Positive and Negative Predictive Value, etc. # Calculate metrics using caret::confusionMatrix confusionMatrix(factor(prediction$response, levels = c(&quot;good&quot;, &quot;bad&quot;)), factor(prediction$observation, levels = c(&quot;good&quot;, &quot;bad&quot;)), mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction good bad ## good 164 10 ## bad 14 273 ## ## Accuracy : 0.9479 ## 95% CI : (0.9235, 0.9664) ## No Information Rate : 0.6139 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.8897 ## ## Mcnemar&#39;s Test P-Value : 0.5403 ## ## Sensitivity : 0.9213 ## Specificity : 0.9647 ## Pos Pred Value : 0.9425 ## Neg Pred Value : 0.9512 ## Precision : 0.9425 ## Recall : 0.9213 ## F1 : 0.9318 ## Prevalence : 0.3861 ## Detection Rate : 0.3557 ## Detection Prevalence : 0.3774 ## Balanced Accuracy : 0.9430 ## ## &#39;Positive&#39; Class : good ## 7.5 Plotting number of cells that pass QC Next, we plot the number of cells in the prostate samples that pass this RandomForest QC. First we plot Patient 3. # Load all profiles profiles = readRDS(&quot;./data/P3_cnv.rds&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = FALSE, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;tissue_type&quot;)) # Get number of counts per region profiles$stats[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] counts = profiles$stats[classifier_prediction == &quot;good&quot;, .N, by = library] # Merge with annotation counts = merge(counts, annot, by = &quot;library&quot;) # Add coordinates counts[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] counts[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Plot ggplot(counts, aes(x = x, y = y, fill = N, label = N)) + geom_tile() + geom_text() + scale_fill_distiller(name = &quot;Number of cells\\npass QC&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limits = c(1, 384)) + geom_hline(yintercept = seq(from = .5, to = max(counts$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(counts$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(counts$y)), labels = seq(1, max(counts$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(counts$x)), labels = seq(1, max(counts$x))) + theme(axis.title = element_blank()) Now, plot the same for Patient 6. # Load all profiles profiles = readRDS(&quot;./data/P6_cnv.rds&quot;) annot = fread(&quot;./annotation/P6.tsv&quot;, header = FALSE, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;tissue_type&quot;)) # Get number of counts per region profiles$stats[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] counts = profiles$stats[classifier_prediction == &quot;good&quot;, .N, by = library] # Merge with annotation counts = merge(counts, annot, by = &quot;library&quot;) # Add coordinates counts[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] counts[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Plot ggplot(counts, aes(x = x, y = y, fill = N, label = N)) + geom_tile() + geom_text() + scale_fill_distiller(name = &quot;Number of cells\\npass QC&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limits = c(1, 384)) + geom_hline(yintercept = seq(from = .5, to = max(counts$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(counts$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(counts$y)), labels = seq(1, max(counts$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(counts$x)), labels = seq(1, max(counts$x))) + theme(axis.title = element_blank()) Next, we plot the number of QC-pass cells for the ACT and scCUTseq comparison. # Load files l9c1 = readRDS(&quot;./data/CD5p_cnv.rds&quot;)$stats l9c2 = readRDS(&quot;./data/CD7p_cnv.rds&quot;)$stats l8c1 = readRDS(&quot;./data/CD4p_cnv.rds&quot;)$stats l2c2 = readRDS(&quot;./data/CD2p_cnv.rds&quot;)$stats l8c2 = readRDS(&quot;./data/CD6p_cnv.rds&quot;)$stats l3c4 = readRDS(&quot;./data/CD3p_cnv.rds&quot;)$stats p7l3c4_cut = readRDS(&quot;./data/CD27.rds&quot;)$stats p7l3c4_act = readRDS(&quot;./data/CD1p.rds&quot;)$stats # Add information and rbind l9c1[, library := &quot;L9C1&quot;] l9c2[, library := &quot;L9C2&quot;] l8c1[, library := &quot;L8C1&quot;] l2c2[, library := &quot;L2C2&quot;] l8c2[, library := &quot;L8C2&quot;] l3c4[, library := &quot;L3C4&quot;] p7l3c4_act[, library := &quot;P7L3C4&quot;] p7l3c4_cut[, library := &quot;P7L3C4&quot;] p7l3c4_cut[, method := &quot;scCUTseq&quot;] act = rbindlist(list(l9c1, l9c2, l8c1, l2c2, l8c2, l3c4, p7l3c4_act)) act[, method := &quot;ACT&quot;] # Load scCUTseq sccutseq = readRDS(&quot;./data/P3_cnv.rds&quot;)$stats annot = fread(&quot;./annotation/P3.tsv&quot;, col.names = c(&quot;lib&quot;, &quot;library&quot;, &quot;Pathology&quot;)) # Extract scCUTseq libraries sccutseq[, lib := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] # Merge with annotation sccutseq = merge(sccutseq, annot, by = &quot;lib&quot;) sccutseq_subset = sccutseq[library %in% c(&quot;L9C1&quot;, &quot;L9C2&quot;, &quot;L8C1&quot;, &quot;L2C2&quot;, &quot;L8C2&quot;, &quot;L3C4&quot;)] sccutseq_subset[, method := &quot;scCUTseq&quot;] # Combine all data total = rbindlist(list(act[, .(classifier_prediction, library, method)], sccutseq_subset[, .(classifier_prediction, library, method)], p7l3c4_cut[, .(classifier_prediction, library, method)])) counts = total[classifier_prediction == &quot;good&quot;, .N / 384, by = .(library, method)] ggplot(counts, aes(x = method, y = V1, group = library)) + geom_point(size = 3) + geom_line() + scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) + labs(y = &quot;Percentage of\\nhigh quality cells&quot;, x = &quot;Method used&quot;) Finally, we plot the number of QC-pass cells in different samples. # Prostate p3 = readRDS(&quot;./data/P3_cnv.rds&quot;)$stats p6 = readRDS(&quot;./data/P6_cnv.rds&quot;)$stats # BRCA brca1 = readRDS(&quot;./data/brca1_cnv.rds&quot;)$stats brca2 = readRDS(&quot;./data/brca2_cnv.rds&quot;)$stats # Brain dg20 = readRDS(&quot;./data/DG20_cnv.rds&quot;)$stats dg21 = readRDS(&quot;./data/DG21_cnv.rds&quot;)$stats dg22 = readRDS(&quot;./data/DG22_cnv.rds&quot;)$stats dg33 = readRDS(&quot;./data/DG33_cnv.rds&quot;)$stats dg39 = readRDS(&quot;./data/DG39_cnv.rds&quot;)$stats dg40 = readRDS(&quot;./data/DG40_cnv.rds&quot;)$stats annot = data.table(library = c(&quot;dg20&quot;, &quot;dg21&quot;, &quot;dg22&quot;, &quot;dg33&quot;, &quot;dg39&quot;, &quot;dg40&quot;), celltype = c(&quot;neun+&quot;, &quot;neun-&quot;, &quot;skmuscle&quot;, &quot;neun-&quot;, &quot;neun+&quot;, &quot;skmuscle&quot;)) # Prepare brain samples dg20[, library := &quot;dg20&quot;] dg21[, library := &quot;dg21&quot;] dg22[, library := &quot;dg22&quot;] dg33[, library := &quot;dg33&quot;] dg39[, library := &quot;dg39&quot;] dg40[, library := &quot;dg40&quot;] brain = rbindlist(list(dg20, dg21, dg22, dg33, dg39, dg40)) # Preare brain and prostate p3[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] p6[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] brca1[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] brca2[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] # Loop through samples and get counts list_samples = c(&quot;p3&quot;, &quot;p6&quot;, &quot;brca1&quot;, &quot;brca2&quot;,&quot;brain&quot;) res = lapply(list_samples, function(sample) { dt = get(sample) counts = data.table(sample = sample, dt[classifier_prediction == &quot;good&quot;, .N, by = library]) counts[, fraction := N / 384] }) res = rbindlist(res) # TK6 tk6 = fread(&quot;./data/tk6_hq_stats.tsv&quot;) all_counts = rbind(res, tk6) # Plot ggplot(all_counts, aes(x = sample, y = fraction)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = .25, size = 3) + labs(y = &quot;Fraction of high quality cells&quot;, x = &quot;&quot;) "],["copy-number-profiles-of-brain-neurons-and-skeletal-muscle-cells.html", "8 Copy number profiles of brain neurons and skeletal muscle cells 8.1 Plot genomewide heatmaps", " 8 Copy number profiles of brain neurons and skeletal muscle cells This sections produces all the figures used in Supplementary Figure 6. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 8.1 Plot genomewide heatmaps First load data and then plot genomewide data using the plotHeatmap() function # Load all profiles dg20 = readRDS(&quot;./data/DG20_cnv.rds&quot;) dg21 = readRDS(&quot;./data/DG21_cnv.rds&quot;) dg22 = readRDS(&quot;./data/DG22_cnv.rds&quot;) dg33 = readRDS(&quot;./data/DG33_cnv.rds&quot;) dg39 = readRDS(&quot;./data/DG39_cnv.rds&quot;) dg40 = readRDS(&quot;./data/DG40_cnv.rds&quot;) # Go through each library and select HQ cells dg20_cn = dg20$copynumber[, dg20$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] dg21_cn = dg21$copynumber[, dg21$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] dg22_cn = dg22$copynumber[, dg22$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] dg33_cn = dg33$copynumber[, dg33$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] dg39_cn = dg39$copynumber[, dg39$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] dg40_cn = dg40$copynumber[, dg40$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] # Add prefix to generate unique colnames setnames(dg20_cn, paste0(&quot;dg20_&quot;, colnames(dg20_cn))) setnames(dg21_cn, paste0(&quot;dg21_&quot;, colnames(dg21_cn))) setnames(dg22_cn, paste0(&quot;dg22_&quot;, colnames(dg22_cn))) setnames(dg33_cn, paste0(&quot;dg33_&quot;, colnames(dg33_cn))) setnames(dg39_cn, paste0(&quot;dg39_&quot;, colnames(dg39_cn))) setnames(dg40_cn, paste0(&quot;dg40_&quot;, colnames(dg40_cn))) # Combine libraries with same cell type donor1_neun = cbind(dg20_cn, dg21_cn) donor1_skmusc = dg22_cn donor2_neun = cbind(dg33_cn, dg39_cn) donor2_skmusc = dg40_cn # Make annotation for brain cells donor1_annot = data.table(sample = colnames(donor1_neun), variable = &quot;celltype&quot;, value = c(rep(&quot;NeuN+&quot;, ncol(dg20_cn)), rep(&quot;NeuN-&quot;, ncol(dg21_cn)))) donor2_annot = data.table(sample = colnames(donor2_neun), variable = &quot;celltype&quot;, value = c(rep(&quot;NeuN&quot;, ncol(dg33_cn)), rep(&quot;NeuN+&quot;, ncol(dg39_cn)))) # Plot heatmaps for donor 1 plotHeatmap(donor1_neun, dg20$bins, annotation = donor1_annot, dendrogram = FALSE) plotHeatmap(donor1_skmusc, dg20$bins, dendrogram = FALSE) # Plot heatmaps for donor 2 plotHeatmap(donor2_neun, dg20$bins, annotation = donor2_annot, dendrogram = FALSE) plotHeatmap(donor2_skmusc, dg20$bins, dendrogram = FALSE) "],["copy-number-profiles-of-other-prostate-cancer-patients.html", "9 Copy number profiles of other prostate cancer patients 9.1 Plot genomewide heatmap 9.2 Plot cell type distribution", " 9 Copy number profiles of other prostate cancer patients This sections produces all the figures used in Supplementary Figure 7. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 9.1 Plot genomewide heatmap First we look at the overall copynumber profiles of the other 4 prostate cancer patients. We sequenced 2 sections (96 cells input) for each patient. # Load data P2 = readRDS(&quot;./data/P2_cnv.rds&quot;) P4 = readRDS(&quot;./data/P4_cnv.rds&quot;) P5 = readRDS(&quot;./data/P5_cnv.rds&quot;) P7 = readRDS(&quot;./data/P7_cnv.rds&quot;) # Select HQ cells p2_profiles = P2$copynumber[, P2$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] p4_profiles = P4$copynumber[, P4$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] p5_profiles = P5$copynumber[, P5$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] p7_profiles = P7$copynumber[, P7$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] # Set blocks to block 1 and 2 setnames(p2_profiles, gsub(&quot;L6C3&quot;, &quot;P2_block1&quot;, colnames(p2_profiles))) setnames(p2_profiles, gsub(&quot;P2L2C3&quot;, &quot;P2_block2&quot;, colnames(p2_profiles))) setnames(p4_profiles, gsub(&quot;L6C4&quot;, &quot;P4_block1&quot;, colnames(p4_profiles))) setnames(p4_profiles, gsub(&quot;L2C3&quot;, &quot;P4_block2&quot;, colnames(p4_profiles))) setnames(p5_profiles, gsub(&quot;L7C5&quot;, &quot;P5_block1&quot;, colnames(p5_profiles))) setnames(p5_profiles, gsub(&quot;L5C5&quot;, &quot;P5_block2&quot;, colnames(p5_profiles))) setnames(p7_profiles, gsub(&quot;L7C3&quot;, &quot;P7_block1&quot;, colnames(p7_profiles))) setnames(p7_profiles, gsub(&quot;L3C3&quot;, &quot;P7_block2&quot;, colnames(p7_profiles))) # Combine data total = do.call(cbind, list(p2_profiles, p4_profiles, p5_profiles, p7_profiles)) bins = P2$bins # Get annotation annot = data.table(sample = colnames(total), patient = gsub(&quot;_.*&quot;, &quot;&quot;, colnames(total)), block = gsub(&quot;P._|_.*&quot;, &quot;&quot;, colnames(total))) annot_m = melt(annot, id.vars = &quot;sample&quot;) # Plot heatmap plotHeatmap(total, bins, annotation = annot_m, linesize = 1.5, rasterize = TRUE) 9.2 Plot cell type distribution Next, we plot the distribution of the three different cell types that we detected earlier in the other 2 prostate cancer samples. # Set diploid vector to compare against diploid_vector = c(rep(2, sum(bins$chr != &quot;X&quot;)), rep(1, sum(bins$chr == &quot;X&quot;))) # Keep these profiles diploid_check = sapply(colnames(total), function(cell) { sum(total[[cell]] != diploid_vector) }) dt = data.table(sample = names(diploid_check), num_altered = diploid_check) # Annotate celltype dt[num_altered == 0, celltype := &quot;diploid&quot;] dt[num_altered != 0 &amp; num_altered &lt;= 0.25 * nrow(bins), celltype := &quot;pseudo-diploid&quot;] dt[num_altered &gt; 0.25 * nrow(bins), celltype := &quot;monster&quot;] # Get patient info dt[, patient := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] counts = dt[, .N, by = .(celltype, patient)] counts_patient = dt[, .N, by = patient] # Get fraction counts = merge(counts, counts_patient, by = &quot;patient&quot;) counts[, fraction := N.x / N.y] # Set order counts[, celltype := factor(celltype, levels = c(&quot;diploid&quot;, &quot;pseudo-diploid&quot;, &quot;monster&quot;))] ggplot(counts, aes(x = patient, y = fraction, fill = celltype, label = N.x)) + geom_col() + scale_y_continuous(expand = c(0, 0), labels = scales::percent_format()) + labs(y = &quot;Percentage of cell type&quot;, x = &quot;&quot;, fill = &quot;Cell type&quot;) + geom_text(aes(label = paste0(round(fraction * 100, 1), &quot;%\\n(n = &quot;, N.x, &quot;)&quot;)), position = position_stack(vjust = .5), size = 4) + scale_fill_npg() + theme(axis.ticks.x = element_blank()) "],["nebnext-versus-sccutseq.html", "10 NEBNext versus scCUTseq 10.1 Validating detected deletions using bulk NEBNext sequencing", " 10 NEBNext versus scCUTseq This sections produces all the figures used in Supplementary Figure 8. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) source(&quot;./functions/plotProfile.R&quot;) 10.1 Validating detected deletions using bulk NEBNext sequencing We have also performed bulk NEBNext sequencing of two sections in P6 where we saw a high number of pseudodiploid cells. # Load in data sccutseq = readRDS(&quot;./data/P6_cnv.rds&quot;) annotation = fread(&quot;./annotation/P6.tsv&quot;, header = FALSE) # The NEBNext sequencing we show is performed on section L2C2 and L3C3 libraries_sccut = annotation[V2 %in% c(&quot;L2C2&quot;, &quot;L3C3&quot;), V1] # First plot genomewide heatmaps. sccutseq_profiles = sccutseq$copynumber[, sccutseq$stats[classifier_prediction == &quot;good&quot;, sample], with = FALSE] sccutseq_profiles_l2c2 = sccutseq_profiles[, grepl(libraries_sccut[1], colnames(sccutseq_profiles)), with = F] sccutseq_profiles_l3c3 = sccutseq_profiles[, grepl(libraries_sccut[2], colnames(sccutseq_profiles)), with = F] # Plot heatmaps # L2C3 plotHeatmap(sccutseq_profiles_l2c2, sccutseq$bins, linesize = 3) #L3C3 plotHeatmap(sccutseq_profiles_l3c3, sccutseq$bins, linesize = 3) # Load NEB data NEB = readRDS(&quot;./data/BC282_cnv.rds&quot;) # Select samples samples = c(&quot;NZ279&quot;, &quot;NZ280&quot;) #NZ279 is L2C2 and NZ280 is L3C3 # Plot profiles # L2C2 plotProfile(NEB$segments[[samples[1]]], NEB$counts_lrr[[samples[[1]]]], bins = NEB$bins, sc = FALSE) # L3C3 plotProfile(NEB$segments[[samples[1]]], NEB$counts_lrr[[samples[[1]]]], bins = NEB$bins, sc = FALSE) "],["whole-genome-sequencing-of-blood.html", "11 Whole-Genome Sequencing of blood 11.1 Plotting WGS data for P3 and P6 blood", " 11 Whole-Genome Sequencing of blood This sections produces all the figures used in Supplementary Figure 9. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotProfile.R&quot;) 11.1 Plotting WGS data for P3 and P6 blood # Load data cnv = readRDS(&quot;./data/blood_wgs_cnv.rds&quot;) # Plot profiles # P3 plotProfile(cnv$segments[[1]], cnv$counts_lrr[[1]], bins = cnv$bins, sc = FALSE) # P6 plotProfile(cnv$segments[[2]], cnv$counts_lrr[[2]], bins = cnv$bins, sc = FALSE) "],["clonal-analysis-pseudo-diploid-cells.html", "12 Clonal analysis pseudo-diploid cells 12.1 Running MEDICC2 12.2 Plot MEDICC2 results 12.3 Identify tumour subclones 12.4 Calculate SCNAs per subclones 12.5 Calculate entropy per subclone", " 12 Clonal analysis pseudo-diploid cells This section produces all the figures used for Figure 2. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 12.1 Running MEDICC2 To run MEDICC2 on the pseudo-diploid cells we have to prepare the data. We won’t save this table here, but you can save this and use this as input for MEDICC2. We only show how to run this for P3, but this is the exact same for P6, simply change the input while you load in the data. Because these steps take some time, we don’t actually run this (eval = FALSE) in this Rmarkdown file. # Load in data dt = readRDS(&quot;./data/P3_pseudodiploid_500kb.rds&quot;) # Melt dt_m = melt(dt, id.vars = c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;)) # Reduce data table dt_short = dt_m[, as.data.table(reduce(IRanges(start, end))), by = .(chr, variable, value)] dt_short = dcast(dt_short, chr + start + end + width ~ variable) # Fill NAs dt_short = dt_short %&gt;% group_by(chr) %&gt;% fill(names(.), .direction = &quot;updown&quot;) %&gt;% ungroup() %&gt;% setDT() # Add diploid dt_short[, diploid := 2] dt_short[chr == &quot;X&quot;, diploid := 1] # Get unique start/end sites (unique based on start site) and go back to long format dt_final = unique(dt_short, by = c(&quot;chr&quot;, &quot;start&quot;)) dt_final = melt(dt_final, id.vars = c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;, &quot;width&quot;)) setnames(dt_final, c(&quot;chrom&quot;, &quot;start&quot;, &quot;end&quot;, &quot;width&quot;, &quot;sample_id&quot;, &quot;total_cn&quot;)) # Rename chromosomes dt_final[chrom == &quot;X&quot;, chrom := 23] dt_final[, chrom := paste0(&quot;chr&quot;, chrom)] After we prepared the input for MEDICC2 we can run it. This is down using the following command: medicc2 --plot none -j {nthreads} --total-copy-numbers -a &#39;total_cn&#39; {input} {output.dir} 12.2 Plot MEDICC2 results After running MEDICC2 we can plot the tree that has been generated alongside the scCUTseq genomewide heatmaps. We load in the previously generated input data and the phylogenetic Newick tree from MEDICC2. First we do this for P3. # Load in data profiles = fread(&quot;./data/phylotrees/P3_input.tsv&quot;) tree = read.tree(&quot;./data/phylotrees/P3_500kb.new&quot;) # Reorder profiles dt = dcast(profiles, chrom + start + end ~ sample_id) dt = dt[gtools::mixedorder(chrom),] # Rename chromosomes setnames(dt, &quot;chrom&quot;, &quot;chr&quot;) dt[chr == &quot;chr23&quot;, chr := &quot;chrX&quot;] # Plot tree tree_plot = ggtree(tree) # Plot heatmap col_order = rev(get_taxa_name(tree_plot)) heatmap = plotHeatmap(dt[, 4:ncol(dt)], dt[, 1:3], dendrogram = F, order = col_order, linesize = .75) # Combine plots tree_plot + heatmap + plot_layout(widths = c(1, 3)) Repeat for P6. # Load in data profiles = fread(&quot;./data/phylotrees/P6_input.tsv&quot;) tree = read.tree(&quot;./data/phylotrees/P6_500kb.new&quot;) # Reorder profiles dt = dcast(profiles, chrom + start + end ~ sample_id) dt = dt[gtools::mixedorder(chrom),] # Rename chromosomes setnames(dt, &quot;chrom&quot;, &quot;chr&quot;) dt[chr == &quot;chr23&quot;, chr := &quot;chrX&quot;] # Plot tree tree_plot = ggtree(tree) # Plot heatmap col_order = rev(get_taxa_name(tree_plot)) heatmap = plotHeatmap(dt[, 4:ncol(dt)], dt[, 1:3], dendrogram = F, order = col_order, linesize = .75) # Combine plots tree_plot + heatmap + plot_layout(widths = c(1, 3)) 12.3 Identify tumour subclones After we generated the phylogenetic tree using MEDICC2, we perform clustering upon this tree. To do this we run TreeCluster. We run the following commands for P3 and P6, respectively. TreeCluster.py -i {input_newick_tree_P3} -t 3 -m max -o {output_clusters_P3} # These are the parameters for P3 TreeCluster.py -i {input_newick_tree_P6} -t 4 -m max -o {output_clusters_P6} # These are the parameters for P6 Next, we want to get the median copy number profile for each subclone. Sometimes it happens that there will be two subclones identified that will have the exact same median copy number profiles, if this is the case, we merge them together. Furthermore, we exclude any subclone that has fewer than 5 cells. The final list of the cell_ids and associated subclone and median copynumber profiles per subclone can be find in the ./data/subclones/ folder. 12.4 Calculate SCNAs per subclones Calculate the percentage of the genome that is altered for P3. profiles = fread(&quot;./data/subclones/P3_median_cn.tsv&quot;) # Get percentage of AMP/DEL/NEUTRAL profiles = dcast(profiles, chr + start + end ~ cluster, value.var = &quot;total_cn&quot;) clones = colnames(profiles[, 4:ncol(profiles)]) res = lapply(clones, function(clone) { # Without X amp_sum = sum(profiles[chr != &quot;X&quot;, ..clone] &gt; 2) del_sum = sum(profiles[chr != &quot;X&quot;, ..clone] &lt; 2) neut_sum = sum(profiles[chr != &quot;X&quot;, ..clone] == 2) # With X amp_sum = amp_sum + sum(profiles[chr == &quot;X&quot;, ..clone] &gt; 1) del_sum = del_sum + sum(profiles[chr == &quot;X&quot;, ..clone] &lt; 1) neut_sum = neut_sum + sum(profiles[chr == &quot;X&quot;, ..clone] == 1) return(data.table(clone = clone, gain = amp_sum / nrow(profiles), loss = del_sum / nrow(profiles), neutral = neut_sum / nrow(profiles))) }) # Rbind results results = rbindlist(res) results_m = melt(results, id.vars = &quot;clone&quot;) # Remove n = results_m[, clone := gsub(&quot; .*&quot;, &quot;&quot;, clone)] # Set factor order results_m[, variable := factor(variable, levels = c(&quot;gain&quot;, &quot;neutral&quot;, &quot;loss&quot;))] results_m[, clone := factor(clone, levels = gtools::mixedsort(unique(clone)))] # Plot ggplot(results_m, aes(x = clone, y = value, fill = variable)) + geom_col(color = &quot;black&quot;) + scale_fill_manual(values = c(&quot;neutral&quot; = &quot;white&quot;, &quot;gain&quot; = &quot;firebrick3&quot;, &quot;loss&quot; = &quot;steelblue&quot;), label = c(&quot;loss&quot;, &quot;neutral&quot;, &quot;gain&quot;), breaks = c(&quot;loss&quot;, &quot;neutral&quot;, &quot;gain&quot;)) + scale_y_continuous(expand = c(0, 0)) + scale_x_discrete(expand = c(0, 0)) + labs(y = &quot;Percentage of genome&quot;, x = &quot;&quot;, fill = &quot;&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + coord_flip() Calculate the percentage of the genome that is altered for P6. profiles = fread(&quot;./data/subclones/P6_median_cn.tsv&quot;) # Get percentage of AMP/DEL/NEUTRAL profiles = dcast(profiles, chr + start + end ~ cluster, value.var = &quot;total_cn&quot;) clones = colnames(profiles[, 4:ncol(profiles)]) res = lapply(clones, function(clone) { # Without X amp_sum = sum(profiles[chr != &quot;X&quot;, ..clone] &gt; 2) del_sum = sum(profiles[chr != &quot;X&quot;, ..clone] &lt; 2) neut_sum = sum(profiles[chr != &quot;X&quot;, ..clone] == 2) # With X amp_sum = amp_sum + sum(profiles[chr == &quot;X&quot;, ..clone] &gt; 1) del_sum = del_sum + sum(profiles[chr == &quot;X&quot;, ..clone] &lt; 1) neut_sum = neut_sum + sum(profiles[chr == &quot;X&quot;, ..clone] == 1) return(data.table(clone = clone, gain = amp_sum / nrow(profiles), loss = del_sum / nrow(profiles), neutral = neut_sum / nrow(profiles))) }) # Rbind results results = rbindlist(res) results_m = melt(results, id.vars = &quot;clone&quot;) # Remove n = results_m[, clone := gsub(&quot; .*&quot;, &quot;&quot;, clone)] # Set factor order results_m[, variable := factor(variable, levels = c(&quot;gain&quot;, &quot;neutral&quot;, &quot;loss&quot;))] results_m[, clone := factor(clone, levels = gtools::mixedsort(unique(clone)))] # Plot ggplot(results_m, aes(x = clone, y = value, fill = variable)) + geom_col(color = &quot;black&quot;) + scale_fill_manual(values = c(&quot;neutral&quot; = &quot;white&quot;, &quot;gain&quot; = &quot;firebrick3&quot;, &quot;loss&quot; = &quot;steelblue&quot;), label = c(&quot;loss&quot;, &quot;neutral&quot;, &quot;gain&quot;), breaks = c(&quot;loss&quot;, &quot;neutral&quot;, &quot;gain&quot;)) + scale_y_continuous(expand = c(0, 0)) + scale_x_discrete(expand = c(0, 0)) + labs(y = &quot;Percentage of genome&quot;, x = &quot;&quot;, fill = &quot;&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + coord_flip() 12.5 Calculate entropy per subclone To assess whether subclones are highly localized in the prostate or widespread we calculate the entropy per subclone. We first normalize the number of cells from each subclone in each section based on the total number of cells that pass QC in that section. Then we calculate the 2D Entropy. # Load in annotation and clusters and cn profiles annot = fread(&quot;./annotation/P3.tsv&quot;, header = F, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;tissue_type&quot;)) clusters = fread(&quot;./data/subclones/P3_clones.tsv&quot;) # Get library info and merge clusters[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] total = merge(clusters, annot, all.x = T) # Get counts per region and total per region counts = total[, .N, by = .(section, cluster)] counts_total = total[, .(total = .N), by = cluster] # Merge and get fraction of total counts = merge(counts, counts_total) counts[, fraction := N / total] counts[, cluster := as.character(cluster)] # Add coordinates counts[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] counts[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] counts[, name := paste0(cluster, &quot; (n = &quot;, total, &quot;)&quot;)] # Normalize for total per section counts_overall = counts[, .(total_cells = sum(N)), by = .(section, x, y)] counts = merge(counts, counts_overall, by = c(&quot;section&quot;, &quot;x&quot;, &quot;y&quot;)) counts[, normalized_count := N / total_cells] res = lapply(unique(counts$name), function(clone) { # Complete dt = counts[name == clone] fill_dt = data.table(cluster = gsub(&quot;c| .*&quot;, &quot;&quot;, clone), section = annot[!section %in% dt$section, section], N = NA, fraction = NA, total = unique(dt$total), normalized_count = NA, total_cells = NA, name = clone) fill_dt[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] fill_dt[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] dt = rbind(dt, fill_dt) mat = as.matrix(dcast(dt, y~x, value.var = &quot;normalized_count&quot;)) mat = mat[, 2:ncol(mat)] mat[is.na(mat)] = 0 return(data.table(name = clone, entropy = Entropy(mat))) }) # Rbind results result = rbindlist(res) result[, clone := gsub(&quot; .*&quot;, &quot;&quot;, name)] setorder(result, entropy) # Plot P3 ggplot(result, aes(x = &quot;&quot;, y = entropy, label = clone)) + geom_quasirandom(size = 5) + labs(y = &quot;Entropy&quot;, x = &quot;&quot;) + geom_text_repel(data = result[1:5], size = 5) And, once again, repeat this for P6. # Load in annotation and clusters and cn profiles annot = fread(&quot;./annotation/P6.tsv&quot;, header = F, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;tissue_type&quot;)) clusters = fread(&quot;./data/subclones/P6_clones.tsv&quot;) # Get library info and merge clusters[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] total = merge(clusters, annot, all.x = T) # Get counts per region and total per region counts = total[, .N, by = .(section, cluster)] counts_total = total[, .(total = .N), by = cluster] # Merge and get fraction of total counts = merge(counts, counts_total) counts[, fraction := N / total] counts[, cluster := as.character(cluster)] # Add coordinates counts[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] counts[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] counts[, name := paste0(cluster, &quot; (n = &quot;, total, &quot;)&quot;)] # Normalize for total per section counts_overall = counts[, .(total_cells = sum(N)), by = .(section, x, y)] counts = merge(counts, counts_overall, by = c(&quot;section&quot;, &quot;x&quot;, &quot;y&quot;)) counts[, normalized_count := N / total_cells] res = lapply(unique(counts$name), function(clone) { # Complete dt = counts[name == clone] fill_dt = data.table(cluster = gsub(&quot;c| .*&quot;, &quot;&quot;, clone), section = annot[!section %in% dt$section, section], N = NA, fraction = NA, total = unique(dt$total), normalized_count = NA, total_cells = NA, name = clone) fill_dt[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] fill_dt[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] dt = rbind(dt, fill_dt) mat = as.matrix(dcast(dt, y~x, value.var = &quot;normalized_count&quot;)) mat = mat[, 2:ncol(mat)] mat[is.na(mat)] = 0 return(data.table(name = clone, entropy = Entropy(mat))) }) # Rbind results result = rbindlist(res) result[, clone := gsub(&quot; .*&quot;, &quot;&quot;, name)] setorder(result, entropy) # Plot P6 ggplot(result, aes(x = &quot;&quot;, y = entropy, label = clone)) + geom_quasirandom(size = 5) + labs(y = &quot;Entropy&quot;, x = &quot;&quot;) + geom_text_repel(data = result[1:5], size = 5) Finally, the heatmaps of the distribution of each subclone (Figure 2k, l), can be found in the Supplementary Figure 12-15 section. "],["phylogenetic-analysis-of-breast-cancer.html", "13 Phylogenetic analysis of breast cancer 13.1 Performing phylogenetic analysis 13.2 Plotting phylogenetic tree and heatmap", " 13 Phylogenetic analysis of breast cancer This sections produces all the figures used in Supplementary Figure 10. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 13.1 Performing phylogenetic analysis We performed phylogenetic analysis of the single-cells of two breast cancer samples using MEDICC2. The input for this the total copy number profiles and the following command was used to run this. medicc2 --plot none -j {nthreads} --total-copy-numbers -a &#39;total_cn&#39; {input_tsv} {output_dir} 13.2 Plotting phylogenetic tree and heatmap We then plot the tree generated by MEDICC2 alongside the single-cell genomewide heatmap. First we plot sample 1. # Load in brca1 data tree = read.tree(&quot;./data/phylotrees/brca1_500kb.new&quot;) input = fread(&quot;./data/phylotrees/brca1_input.tsv&quot;) # Reorder dt = dcast(input, chrom + start + end ~ sample_id) dt = dt[gtools::mixedorder(chrom),] # Rename column and set chr23 back to chrX setnames(dt, &quot;chrom&quot;, &quot;chr&quot;) dt[chr == &quot;chr23&quot;, chr := &quot;chrX&quot;] # include diploid dt[, diploid := 2L] dt[chr == &quot;chrX&quot;, diploid := 1L] # Plot tree tree_plot = ggtree(tree) # Get sample order from tree col_order = rev(get_taxa_name(tree_plot)) # Plot heatmap heatmap = plotHeatmap(dt[, 4:ncol(dt)], dt[, 1:3], dendrogram = F, order = col_order, linesize = .75) # Combine plots tree_plot + heatmap + plot_layout(widths = c(1, 3)) Next, we do the same but for sample 2. # Load in brca1 data tree = read.tree(&quot;./data/phylotrees/brca2_500kb.new&quot;) input = fread(&quot;./data/phylotrees/brca2_input.tsv&quot;) # Reorder dt = dcast(input, chrom + start + end ~ sample_id) dt = dt[gtools::mixedorder(chrom),] # Rename column and set chr23 back to chrX setnames(dt, &quot;chrom&quot;, &quot;chr&quot;) dt[chr == &quot;chr23&quot;, chr := &quot;chrX&quot;] # include diploid dt[, diploid := 2L] dt[chr == &quot;chrX&quot;, diploid := 1L] # Plot tree tree_plot = ggtree(tree) # Get sample order from tree col_order = rev(get_taxa_name(tree_plot)) # Plot heatmap heatmap = plotHeatmap(dt[, 4:ncol(dt)], dt[, 1:3], dendrogram = F, order = col_order, linesize = .75) # Combine plots tree_plot + heatmap + plot_layout(widths = c(1, 3)) "],["subclonal-distribution-in-prostate.html", "14 Subclonal distribution in prostate 14.1 Plotting spatial subclonal distribution", " 14 Subclonal distribution in prostate This sections produces all the figures used in Supplementary Figure 11. # Source setup file source(&quot;./functions/setup.R&quot;) 14.1 Plotting spatial subclonal distribution We plot the spatial subclonal distribution using piecharts and placing them in the spatial location of the section. The code used to generate the file containing the subclone information is included in Figure 2. We also include the final file in this repository. First we plot P3. # Load data clones = fread(&quot;./data/subclones/P3_clones.tsv&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = F) # Merge clones with annotation data clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) setnames(clones, &quot;V2&quot;, &quot;section&quot;) # Get fraction of each subclone in each section clones = clones[, .N, by = .(cluster, section)] clones = clones[, .(cluster = cluster, fraction = N / sum(N)), by = .(section)] # Add coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs clones = complete(clones, tidyr::expand(clones, x, y)) setDT(clones) clones[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] # Add top 3 indication setorder(clones, section, -fraction) clones[, rankings := 1:.N, by = .(section)] clones[rankings &lt;= 3, label_text := cluster] # Get label positions clones = clones %&gt;% group_by(section) %&gt;% mutate(text_y = cumsum(fraction) - fraction/2) setDT(clones) # Get colors #colors_vector = get_colors() col_vector = createPalette(sum(!is.na(unique(clones$cluster))), c(&quot;#ff0000&quot;, &quot;#00ff00&quot;, &quot;#0000ff&quot;)) col_vector = setNames(col_vector, paste0(&quot;c&quot;, 1:sum(!is.na(unique(clones$cluster))))) # Set sections all_sections = unique(clones$section) # Plot plots = lapply(all_sections, function(subset) { if(is.na(clones[section == subset]$fraction)[1]) { plt = ggplot() + theme_void() } else { plt = ggplot(clones[section == subset], aes(x = &quot;&quot;, y = fraction, fill = cluster)) + geom_col() + coord_polar(theta = &quot;y&quot;) + geom_label_repel(aes(label = label_text), position = position_stack(vjust = .5), size = 6, box.padding = unit(0.6, &quot;lines&quot;)) + scale_fill_manual(values = col_vector) + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), legend.position = &quot;none&quot;) } return(plt) }) # Set names names(plots) = all_sections # Reorder plots based on tumor location order = unique(clones, by = &quot;section&quot;) setorder(order, y, -x) plots = plots[order$section] # Arrange plots cowplot::plot_grid(plotlist = plots, nrow = max(order$y), ncol = max(order$x)) Now we do the same for P6 # Load data clones = fread(&quot;./data/subclones/P6_clones.tsv&quot;) annot = fread(&quot;./annotation/P6.tsv&quot;, header = F) # Merge clones with annotation data clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) setnames(clones, &quot;V2&quot;, &quot;section&quot;) # Get fraction of each subclone in each section clones = clones[, .N, by = .(cluster, section)] clones = clones[, .(cluster = cluster, fraction = N / sum(N)), by = .(section)] # Add coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs clones = complete(clones, tidyr::expand(clones, x, y)) setDT(clones) clones[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] # Add top 3 indication setorder(clones, section, -fraction) clones[, rankings := 1:.N, by = .(section)] clones[rankings &lt;= 3, label_text := cluster] # Get label positions clones = clones %&gt;% group_by(section) %&gt;% mutate(text_y = cumsum(fraction) - fraction/2) setDT(clones) # Get colors #colors_vector = get_colors() col_vector = createPalette(sum(!is.na(unique(clones$cluster))), c(&quot;#ff0000&quot;, &quot;#00ff00&quot;, &quot;#0000ff&quot;)) col_vector = setNames(col_vector, paste0(&quot;c&quot;, 1:sum(!is.na(unique(clones$cluster))))) # Set sections all_sections = unique(clones$section) # Plot plots = lapply(all_sections, function(subset) { if(is.na(clones[section == subset]$fraction)[1]) { plt = ggplot() + theme_void() } else { plt = ggplot(clones[section == subset], aes(x = &quot;&quot;, y = fraction, fill = cluster)) + geom_col() + coord_polar(theta = &quot;y&quot;) + geom_label_repel(aes(label = label_text), position = position_stack(vjust = .5), size = 6, box.padding = unit(0.6, &quot;lines&quot;)) + scale_fill_manual(values = col_vector) + theme(axis.line = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), legend.position = &quot;none&quot;) } return(plt) }) # Set names names(plots) = all_sections # Reorder plots based on tumor location order = unique(clones, by = &quot;section&quot;) setorder(order, y, -x) plots = plots[order$section] # Arrange plots cowplot::plot_grid(plotlist = plots, nrow = max(order$y), ncol = max(order$x)) "],["individual-subclone-distribution-in-prostate.html", "15 Individual subclone distribution in prostate 15.1 Plotting individual subclone distribution", " 15 Individual subclone distribution in prostate This sections produces all the figures used in Supplementary Figure 11, 12, 13 and 14. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) source(&quot;./functions/plotProfile.R&quot;) 15.1 Plotting individual subclone distribution Here we plot, for each detected subclone, the distribution within the prostate. The scripts to obtain the list of subclones and the associated entropy is in the Rmd file for Figure 2. Once again, we do this first for P3. # Load data profiles = readRDS(&quot;./data/P3_pseudodiploid_500kb.rds&quot;) clones = fread(&quot;./data/subclones/P3_clones.tsv&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = F) entropy = fread(&quot;./data/entropy/P3_entropy.tsv&quot;) # Make copy of clones clones_annot = copy(clones) # Prepare cn profiles profiles = profiles[, c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;, clones$sample_id), with = F] # Melt into long format and merge with clone information profiles = melt(profiles, id.vars = c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;)) profiles = merge(profiles, clones, by.x = &quot;variable&quot;, by.y = &quot;sample_id&quot;) # Get consensus profiles profiles = profiles[, .(cn = round(median(value))), by = .(chr, start, end, cluster)] profiles = dcast(profiles, chr+start+end ~ cluster, value.var = &quot;cn&quot;) profiles = profiles[gtools::mixedorder(profiles$chr)] # Merge clones with annotation data clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;, all.y = TRUE) setnames(clones, &quot;V2&quot;, &quot;section&quot;) # Get fraction of each subclone in each section clones = clones[, .N, by = .(cluster, section)] clones = clones[, .(count = N, section = section, fraction = N / sum(N)), by = .(cluster)] # Add coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs clones[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] clones = clones |&gt; complete(section, cluster) setDT(clones) # Readd coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Loop through clones and combine the distribution heatmap and the genomewide heatmap plots = lapply(entropy$clone, function(clone) { # Plot profile heatmap heatmap = plotHeatmap(profiles[, clone, with = F], profiles[, 1:3], linesize = 20, dendrogram = F, order = clone, rast = T) + theme(legend.position = &quot;none&quot;) # Complete dt = clones[cluster == clone] plt = ggplot(dt, aes(x = x, y = y, fill = fraction, label = count)) + geom_tile() + geom_text() + labs(title = clones_annot[cluster == clone, name]) + scale_fill_distiller(name = &quot;Subclone\\nFraction&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limits = c(0, 1)) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank()) # Combine plots combined = wrap_plots(heatmap, plt, ncol = 1, heights = c(0.125, 1)) return(combined) }) # Plot everything at once wrap_plots(plots, ncol = 4) # You can also plot 1 by 1 by not running this but simply running &#39;plots&#39; And now for P6 # Load data profiles = readRDS(&quot;./data/P6_pseudodiploid_500kb.rds&quot;) clones = fread(&quot;./data/subclones//P6_clones.tsv&quot;) annot = fread(&quot;./annotation/P6.tsv&quot;, header = F) entropy = fread(&quot;./data/entropy/P6_entropy.tsv&quot;) # Make copy of clones clones_annot = copy(clones) # Prepare cn profiles profiles = profiles[, c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;, clones$sample_id), with = F] # Melt into long format and merge with clone information profiles = melt(profiles, id.vars = c(&quot;chr&quot;, &quot;start&quot;, &quot;end&quot;)) profiles = merge(profiles, clones, by.x = &quot;variable&quot;, by.y = &quot;sample_id&quot;) # Get consensus profiles profiles = profiles[, .(cn = round(median(value))), by = .(chr, start, end, cluster)] profiles = dcast(profiles, chr+start+end ~ cluster, value.var = &quot;cn&quot;) profiles = profiles[gtools::mixedorder(profiles$chr)] # Merge clones with annotation data clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;, all.y = TRUE) setnames(clones, &quot;V2&quot;, &quot;section&quot;) # Get fraction of each subclone in each section clones = clones[, .N, by = .(cluster, section)] clones = clones[, .(count = N, section = section, fraction = N / sum(N)), by = .(cluster)] # Add coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs clones[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] clones = clones |&gt; complete(section, cluster) setDT(clones) # Readd coordinates clones[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] clones[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Loop through clones and combine the distribution heatmap and the genomewide heatmap plots = lapply(entropy$clone, function(clone) { # Plot profile heatmap heatmap = plotHeatmap(profiles[, clone, with = F], profiles[, 1:3], linesize = 20, dendrogram = F, order = clone, rast = T) + theme(legend.position = &quot;none&quot;) # Complete dt = clones[cluster == clone] plt = ggplot(dt, aes(x = x, y = y, fill = fraction, label = count)) + geom_tile() + geom_text() + labs(title = clones_annot[cluster == clone, name]) + scale_fill_distiller(name = &quot;Subclone\\nFraction&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, limits = c(0, 1)) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank()) # Combine plots combined = wrap_plots(heatmap, plt, ncol = 1, heights = c(0.125, 1)) return(combined) }) # Plot everything at once wrap_plots(plots, ncol = 4) # You can also plot 1 by 1 by not running this but simply running &#39;plots&#39; Now, for the top 5 most local (lowest entropy) subclones, we plot the individual copy number profiles for manual inspection. # Load data cnv = readRDS(&quot;./data/P3_cnv.rds&quot;) clones = fread(&quot;./data/subclones/P3_clones.tsv&quot;) entropy = fread(&quot;./data/entropy/P3_entropy.tsv&quot;) # Select top 5 clones top5 = entropy[1:5, name] # Select profiles top_profiles = clones[name %in% top5, sample_id] # Loop through profiles and plot plots = lapply(top_profiles, function(x) { plt = plotProfile(cnv$copynumber[[x]], cnv$counts_gc[[x]] * cnv$ploidies[sample == x, ploidy], bins = cnv$bins) + ggtitle(paste0(clones[sample_id == x, name], &quot; - &quot;, x)) return(plt) }) wrap_plots(plots, ncol = 3) And repeat for P6. # Load data cnv = readRDS(&quot;./data/P6_cnv.rds&quot;) clones = fread(&quot;./data/subclones/P6_clones.tsv&quot;) entropy = fread(&quot;./data/entropy/P6_entropy.tsv&quot;) # Select top 5 clones top5 = entropy[1:5, name] # Select profiles top_profiles = clones[name %in% top5, sample_id] # Loop through profiles and plot plots = lapply(top_profiles, function(x) { plt = plotProfile(cnv$copynumber[[x]], cnv$counts_gc[[x]] * cnv$ploidies[sample == x, ploidy], bins = cnv$bins) + ggtitle(paste0(clones[sample_id == x, name], &quot; - &quot;, x)) return(plt) }) wrap_plots(plots, ncol = 3) "],["monster-cells.html", "16 Monster cells 16.1 Clustering copynumber profiles 16.2 Plotting genomewide heatmaps 16.3 Plot distribution", " 16 Monster cells This sections produces all the figures used in Supplementary Figure 16 and 17. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 16.1 Clustering copynumber profiles First we cluster the copynumber profiles of the previously identified monster cells. Note that the results might slightly differ from the plots in the manuscript due to random seeds being different. # Load data annot_p3 = fread(&quot;./annotation/P3.tsv&quot;, header = F, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;pathology&quot;)) profiles_p3 = readRDS(&quot;./data/P3_monsters_500kb.rds&quot;) # Run UMAP set.seed(678) #678 umap_res_p3 = umap(t(profiles_p3[, 4:ncol(profiles_p3)]), metric = &quot;manhattan&quot;, min_dist = 0, n_neighbors = 3, spread = 1, n_components = 2) # Transform into dt umap_dt_p3 = data.table(x = umap_res_p3[, 1], y = umap_res_p3[, 2], cell = as.character(1:nrow(umap_res_p3)), sample = colnames(profiles_p3[, 4:ncol(profiles_p3)])) # Run clustering and remove cluster 0 (= noise points) if present clusters_p3 = dbscan::hdbscan(umap_dt_p3[,c(1:2)], minPts = 10) umap_dt_p3[, cluster := factor(clusters_p3$cluster)] umap_dt_p3 = umap_dt_p3[cluster != &quot;0&quot;, ] ggplot(umap_dt_p3, aes(x = x, y = y, label = cluster)) + geom_point(size = 2, alpha = 1, aes(color = cluster)) + scale_color_npg() + labs(x = &quot;UMAP 1&quot;, y = &quot;UMAP 2&quot;) Run the same for P6 # Load data annot_p6 = fread(&quot;./annotation/P6.tsv&quot;, header = F, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;pathology&quot;)) profiles_p6 = readRDS(&quot;./data/P6_monsters_500kb.rds&quot;) # Run UMAP set.seed(678) #678 umap_res_p6 = umap(t(profiles_p6[, 4:ncol(profiles_p6)]), metric = &quot;manhattan&quot;, min_dist = 0, n_neighbors = 3, spread = 1, n_components = 2) # Transform into dt umap_dt_p6 = data.table(x = umap_res_p6[, 1], y = umap_res_p6[, 2], cell = as.character(1:nrow(umap_res_p6)), sample = colnames(profiles_p6[, 4:ncol(profiles_p6)])) # Run clustering and remove cluster 0 (= noise points) if present clusters_p6 = dbscan::hdbscan(umap_dt_p6[,c(1:2)], minPts = 10) umap_dt_p6[, cluster := factor(clusters_p6$cluster)] umap_dt_p6 = umap_dt_p6[cluster != &quot;0&quot;, ] ggplot(umap_dt_p6, aes(x = x, y = y, label = cluster)) + geom_point(size = 2, alpha = 1, aes(color = cluster)) + scale_color_npg() + labs(x = &quot;UMAP 1&quot;, y = &quot;UMAP 2&quot;) 16.2 Plotting genomewide heatmaps Now we plot, for each cluster, the genomewide heatmap to show the copynumber profiles of these cells. # For P3 plots = lapply(unique(umap_dt_p3$cluster), function(clust) { bins = profiles_p3[, 1:3] profiles = profiles_p3[, umap_dt_p3[cluster == clust, sample], with = FALSE] plotHeatmap(profiles, bins, dendrogram = FALSE) }) wrap_plots(plots, ncol = 1, heights = c(0.2, 1)) # For P6 plots = lapply(unique(umap_dt_p6$cluster), function(clust) { bins = profiles_p6[, 1:3] profiles = profiles_p6[, umap_dt_p6[cluster == clust, sample], with = FALSE] plotHeatmap(profiles, bins, dendrogram = FALSE) }) wrap_plots(plots, ncol = 1, heights = c(0.25, 0.6, 0.1, 0.05, 0.05)) 16.3 Plot distribution Finally, we plot the distribution of where these cells are located in the prostate. First we run it for P3. locations = umap_dt_p3[, 4:5] locations[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] locations = merge(locations, annot_p3, by = &quot;library&quot;) # Get counts and fractions locations = locations[, .(count = .N), by = .(section, cluster)] locations = locations[, .(count = count, fraction = count / sum(count), section = section), by = .(cluster)] # Fill locations[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] locations[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs locations = complete(locations, tidyr::expand(locations, x, y)) setDT(locations) locations[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] locations = locations[!is.na(cluster), ] # Plot ggplot(locations, aes(x = x, y = y, fill = fraction, label = count)) + geom_tile() + geom_text() + facet_wrap(~cluster) + scale_fill_distiller(name = &quot;Hopeful-Monster\\nFraction&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;) + geom_hline(yintercept = seq(from = .5, to = max(locations$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(locations$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(locations$y)), labels = seq(1, max(locations$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(locations$x)), labels = seq(1, max(locations$x))) + theme(axis.title = element_blank()) Lastly, we run it for P6. locations = umap_dt_p6[, 4:5] locations[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] locations = merge(locations, annot_p6, by = &quot;library&quot;) # Get counts and fractions locations = locations[, .(count = .N), by = .(section, cluster)] locations = locations[, .(count = count, fraction = count / sum(count), section = section), by = .(cluster)] # Fill locations[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] locations[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill NAs locations = complete(locations, tidyr::expand(locations, x, y)) setDT(locations) locations[, section := paste0(&quot;L&quot;, x, &quot;C&quot;, y)] locations = locations[!is.na(cluster), ] # Plot ggplot(locations, aes(x = x, y = y, fill = fraction, label = count)) + geom_tile() + geom_text() + facet_wrap(~cluster) + scale_fill_distiller(name = &quot;Hopeful-Monster\\nFraction&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;) + geom_hline(yintercept = seq(from = .5, to = max(locations$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(locations$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(locations$y)), labels = seq(1, max(locations$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(locations$x)), labels = seq(1, max(locations$x))) + theme(axis.title = element_blank()) "],["validation-of-pseudodiploid-deletions.html", "17 Validation of pseudodiploid deletions", " 17 Validation of pseudodiploid deletions This sections produces all the figures used in Figure 3. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) # Load in data ACT = readRDS(&quot;./data/CD1p.rds&quot;) scCUTseq = readRDS(&quot;./data/CD27.rds&quot;) # Plot heatmaps # scCUTseq plotHeatmap(scCUTseq$copynumber[, scCUTseq$stats[classifier_prediction == &quot;good&quot;, sample], with = F], scCUTseq$bins, linesize = 1.6) # ACT plotHeatmap(ACT$copynumber[, ACT$stats[classifier_prediction == &quot;good&quot;, sample], with = F], ACT$bins, linesize = 1.6) The analysis and figures based on the microscopy is unfortunately not included in this markdown. "],["trrfer-subclone-analysis.html", "18 TRR/FER subclone analysis 18.1 Plotting deleted genes from TRR/FER subclones 18.2 Co-deletions of TSGs 18.3 TCGA deletion analysis 18.4 Targeted sequencing mutations", " 18 TRR/FER subclone analysis This section produces all the figures used for Figure 4. # Source setup file source(&quot;./functions/setup.R&quot;) # Load functions source(&quot;./functions/plotHeatmap.R&quot;) 18.1 Plotting deleted genes from TRR/FER subclones profiles = fread(&quot;./data/subclones/P3_median_cn.tsv&quot;) clones = fread(&quot;./data/subclones/P3_clones.tsv&quot;) cosmic = fread(&quot;./data/genelists/cosmic_gene-census.tsv&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = F) # Merge clones with annot clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones_annot = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) # Select clones that are only present in cancer/focal total = clones_annot[, .(paste(unique(V3), collapse = &quot;;&quot;)), by = cluster] clones_selected = total[!grepl(&quot;Normal&quot;, V1)] # Get selected profiles = profiles[cluster %in% clones_selected$cluster] profiles = profiles[(total_cn != 2 &amp; chr != &quot;X&quot;) | (total_cn != 1 &amp; chr == &quot;X&quot;)] # Get overlaps setkey(profiles, chr, start, end) setkey(cosmic, chr, start, end) overlaps = foverlaps(cosmic, profiles)[!is.na(total_cn)] overlaps[, alteration := ifelse(total_cn &lt; 2, &quot;Deleted&quot;, &quot;Amplified&quot;)] # Get unique unique_overlaps = unique(overlaps[, .(name, gene, alteration)]) # Make oncoprint unique_overlaps_wide = dcast(unique_overlaps, name ~ gene, value.var = &quot;alteration&quot;) mat = as.matrix(unique_overlaps_wide[, 2:ncol(unique_overlaps_wide)]) rownames(mat) = unique_overlaps_wide$name # Plot oncoprint # colors cols = brewer.pal(3, &quot;Set1&quot;)[c(1, 2)] names(cols) = c(&quot;Amplified&quot;, &quot;Deleted&quot;) oncoPrint(t(mat), alter_fun = list( background = alter_graphic(&quot;rect&quot;, width = 0.9, height = 0.9, fill = &quot;#FFFFFF&quot;, col = &quot;black&quot;, size = .1), Amplified = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.85, fill = cols[&quot;Amplified&quot;]), Deleted = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.85, fill = cols[&quot;Deleted&quot;])), col = cols, border = &quot;black&quot;, show_column_names = T, show_row_names = T, remove_empty_rows = T, show_pct = F, row_names_gp = gpar(fontsize = 17), column_names_gp = gpar(fontsize = 22)) Distributions of the cells belonging to these subclones can be found in the Supplementary Figure 12-15 section. 18.2 Co-deletions of TSGs To see where the cells harbouring deletions of multiple genes simultaneously are located, we go back to single-cell level (from subclonal level) and plot these cells in the prostate heatmaps. # Load in data count_all = readRDS(&quot;./data/P3_cnv.rds&quot;)$stats profiles = readRDS(&quot;./data/P3_pseudodiploid_500kb.rds&quot;) cosmic = fread(&quot;./data/genelists/cosmic_gene-census.tsv&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = F) clones = fread(&quot;./data/subclones/P3_clones.tsv&quot;) # Get cosmic genes that are amp/del dt = pivot_longer(profiles, cols = colnames(profiles[, 4:ncol(profiles)])) setDT(dt) dt = dt[value != 2, ] dt[, value := ifelse(value &lt; 2, &quot;Deleted&quot;, &quot;Amplified&quot;)] # Setkeys setkey(dt, chr, start, end) setkey(cosmic, chr, start, end) # Get overlap with COSMIC overlap = foverlaps(cosmic, dt) overlap = overlap[complete.cases(overlap)] overlap = unique(overlap, by = c(&quot;name&quot;, &quot;value&quot;, &quot;gene&quot;)) # Merge overlap with section info and subclone info overlap[, library := gsub(&quot;_.*&quot;, &quot;&quot;, name)] dt = merge(overlap, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) #dt = merge(dt, clones, by.x = &quot;name&quot;, by.y = &quot;sample_id&quot;) # Subclones and genes to check genes_check = c(&quot;FOXO1&quot;, &quot;FOXO3&quot;, &quot;RB1&quot;, &quot;BRCA2&quot;, &quot;CCNC&quot;, &quot;CDX2&quot;, &quot;LATS2&quot;, &quot;PRDM1&quot;) # Subset dt = dt[gene %in% genes_check, ] # Check co-deletions codels = dt[, paste(gene, collapse = &quot;;&quot;), by = .(name, V2, value)] codels_counts = codels[, .N, by = .(V2, V1)] # Get total number of pseudodiploid per section total_pseudo = data.table(library = gsub(&quot;_.*&quot;, &quot;&quot;, colnames(profiles[, 4:ncol(profiles)]))) total_pseudo = merge(total_pseudo, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) counts_pseudo = total_pseudo[, .(total_pseudo = .N), by = V2] # Get total cells count_all[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample)] count_all = merge(count_all, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) counts_total = count_all[classifier_prediction == &quot;good&quot;, .(total = .N), by = .(V2)] # Get fraction pseudo codels_counts = merge(codels_counts, counts_pseudo, by = &quot;V2&quot;) codels_counts[, fraction_pseudo := N / total_pseudo] # Add fraction total codels_counts = merge(codels_counts, counts_total, by = &quot;V2&quot;) codels_counts[, fraction_total := N / total] # Plot the combination with all deletions combination = &quot;LATS2;CDX2;BRCA2;FOXO1;RB1;CCNC;PRDM1;FOXO3&quot; # Select the combination of interest dt = codels_counts[V1 == combination] # Fill all with NAs fill_dt = data.table(V2 = annot[!V2 %in% dt$V2, V2], V1 = NA, N = NA, total_pseudo = NA, fraction_pseudo = NA, total = NA, fraction_total = NA) dt = rbindlist(list(dt, fill_dt), use.names = TRUE) # Add coordinates dt[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, V2))] dt[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, V2))] # Plot the number of cells with co-deletion ggplot(dt, aes(x = x, y = y, fill = N, label = N)) + geom_tile() + geom_text(size = 7) + labs(title = paste0(combination, &quot; deletions&quot;)) + scale_fill_distiller(name = &quot;Cells with deletion&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank()) # Plot fraction of pseudodiploid cells ggplot(dt, aes(x = x, y = y, fill = fraction_pseudo, label = N)) + geom_tile() + geom_text(size = 7) + labs(title = paste0(combination, &quot; deletions&quot;)) + scale_fill_distiller(name = &quot;fraction of all\\npseudo-diploid cells&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, labels = scales::percent_format(), limits = c(0, 1)) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank()) # Plot fraction of total cells ggplot(dt, aes(x = x, y = y, fill = fraction_total, label = N)) + geom_tile() + geom_text(size = 7) + labs(title = paste0(combination, &quot; deletions&quot;)) + scale_fill_distiller(name = &quot;fraction of\\nall cells&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;, labels = scales::percent_format(), limits = c(0, 1)) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank()) 18.3 TCGA deletion analysis Next, we wanted to see if these deletions are also present (and enriched) in the Prostate Adenocarcinoma (PRAD) TCGA dataset. First we looked into enrichment of deletions of certain genomic regions in TCGA. We used GISTIC2 for this. You can download the TCGA copy number data at cbioportal. Then we ran GISTIC2 with the following command: gistic2 gp_gistic2_from_seg -b {base_dir} -gcm extreme -seg {TCGA_copynumber_segment_file} -maxseg 2000 -broad 1 -ta 0.1 -td 0.1 -conf 0.99 -brlen 0.7 -armpeel 1 We then used the scores.gistic output file from GISTIC2 to visualize the signficantly deleted (and amplified) regions in PRAD. # Load GISTIC data dt = fread(&quot;./data/TCGA/scores.gistic&quot;) # Make wide dt_amp = dt[Type == &quot;Amp&quot;, .(chr = paste0(&quot;chr&quot;, Chromosome), start = Start, end = End, qvalue = -1*`-log10(q-value)`)] dt_del = dt[Type == &quot;Del&quot;, .(chr = paste0(&quot;chr&quot;, Chromosome), start = Start, end = End, qvalue = `-log10(q-value)`)] circos.clear() circos.par(&quot;start.degree&quot; = 90, &quot;gap.degree&quot; = c(rep(1, 21), 10), points.overflow.warning=FALSE, track.height = 0.25, track.margin = c(0.01, 0)) circos.initializeWithIdeogram(plotType = c(&quot;ideogram&quot;, &quot;axis&quot;, &quot;labels&quot;), chromosome.index = paste0(&quot;chr&quot;, 1:22)) # Plot deletion q value track circos.genomicTrack(dt_del, ylim = c(0, 100), panel.fun = function(region, value, ...) { circos.genomicLines(region, value, numeric.column = &quot;del&quot;, col = &quot;#377EB8&quot;, area = TRUE) }) circos.yaxis(side = &quot;left&quot;, at = c(0, 50, 100), labels = c(0, 50, 100), sector.index = &quot;chr1&quot;, labels.niceFacing = TRUE) # Plot amplification q value track circos.genomicTrack(dt_amp, ylim = c(-10, 0), panel.fun = function(region, value, ...) { circos.genomicLines(region, value, numeric.column = &quot;amp&quot;, col = &quot;#E41A1C&quot;, area = TRUE, baseline = &quot;top&quot;, type = &quot;l&quot;) }) circos.yaxis(side = &quot;left&quot;, at = c(-10, -5, 0), labels = c(10, 5, 0), sector.index = &quot;chr1&quot;, labels.niceFacing = TRUE) Next, we checked the mutation-status (deletions/amplfications/SNVs) of the genes that we found commonly deleted in our samples in TCGA PRAD. For this we load in the CNA data (.seg file) and we download the mutation data. Note, the downloading and loading of the mutation data can take a couple of minutes. # Load TCGA data segments = fread(&quot;./data//TCGA/TCGA_firehose-segments.seg&quot;) clinical = fread(&quot;./data/TCGA/TCGA_firehose_clinical_data_patient.txt&quot;) cosmic = fread(&quot;./data/genelists/cosmic_gene-census.tsv&quot;) # Download mutation data res = GDCquery(project = &quot;TCGA-PRAD&quot;, data.category = &quot;Simple Nucleotide Variation&quot;, access = &quot;open&quot;, data.type = &quot;Masked Somatic Mutation&quot;, workflow.type = &quot;Aliquot Ensemble Somatic Variant Merging and Masking&quot;) GDCdownload(res, directory = &quot;./data/TCGA/gdc_query/&quot;) tcga_muts = GDCprepare(res, directory = &quot;./data/TCGA/gdc_query/&quot;) ## [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m15.49MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m955.20TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.71PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.42PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.58PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m710.30TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.19PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.02PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m4.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m3.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.26PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.37PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.38PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m14.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m703.74TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.61PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.51PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m17.78MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m960.67TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m919.20TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.32PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.55PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m989.92TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.59PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.65PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m793.62TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m889.75TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.26PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m41.25TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m982.50TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m948.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m371.34TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m282.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m807.84TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m980.44TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.66PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m796.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.55PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m531.33TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.80PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.19PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.44PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m907.47TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m964.43TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m881.90TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m558.64TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m25.23MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m571.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m719.93TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m725.16TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.84PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.34PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m865.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.32PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.38PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.30PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m920.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.07PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.40PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.79PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.28PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.71PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.55PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.56PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.34PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.33PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m966.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.30PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m612.40TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.52PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m510.94TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m17.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m152.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.36PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.02PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m850.94TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.43PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.15PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m755.87TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.40PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m920.01TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m721.04TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m3.38PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m895.26TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m754.24TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.34PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m944.24TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.91PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.59PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m961.56TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m12.42MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.94PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.70PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.20PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m8.64MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.15PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.36PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m859.84TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m953.25TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m930.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m776.44TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m777.01TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m709.22TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.15PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.20PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.23PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.12PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.24PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m666.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m876.55TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.03PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m799.98TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.78PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.52PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m708.26TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m885.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.41PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.03PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m768.61TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m681.67TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m865.70TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.16PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.48PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m877.84TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m154.77TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.43PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.48PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m834.69TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m862.85TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m798.15TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m765.10TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m454.96TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.51PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m9.10MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.12PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.10PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m948.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m777.59TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.37PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m957.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m512.56TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m710.78TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m656.18TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m911.61TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.19PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.15PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m986.20TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.46PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.97PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m26.52MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m957.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m842.57TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m149.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.28PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.52PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.47PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m819.68TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m716.85TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m500.75TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.39PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m668.41TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m407.65TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m710.30TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m999.12TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.52PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m932.07TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m138.35TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.32PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m759.84TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.28PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m956.95TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.20PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.17PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.57PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m840.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.48PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m881.16TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.20PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m890.51TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m843.08TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m667.14TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m953.25TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m991.09TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m865.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m828.42TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m723.65TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.75PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m820.32TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m158.65TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m668.52TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m763.29TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.03PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m754.64TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m964.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.42PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m17.78MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.07PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.69PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.36PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.31PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.64PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m956.95TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m859.84TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.61PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m778.89TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m900.07TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.38PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.26PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.46PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m12.42MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.57PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.37PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m733.65TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m964.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m860.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m940.85TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m743.01TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m12.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m924.26TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.12PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m812.38TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m943.39TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m998.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m834.02TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m877.10TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m910.62TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.49PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m878.02TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m878.76TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m627.42TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.64PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m641.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m985.27TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m17.78MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m953.47TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.03PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m961.33TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m417.18TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m854.59TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.00PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.56PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.82PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m26.97MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m825.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.47PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m13.37MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m763.99TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.59PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m961.56TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m889.00TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m12.82MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m947.01TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.02PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.39PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m841.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m814.90TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m948.72TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m889.00TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m984.12TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.56PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m815.70TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m640.16TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m972.03TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m668.41TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m911.61TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m810.96TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m723.16TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.16PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.37PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m932.90TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m957.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m598.42TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m964.43TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m633.39TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m651.49TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m707.66TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.50PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m947.87TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m404.39TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.15PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m864.27TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m967.10TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m826.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.45PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m826.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m13.63MB[0m in [36m 0s[0m, [32m67.92MB/s[0m [1mindexed[0m [32m13.76MB[0m in [36m 0s[0m, [32m68.06MB/s[0m [1mindexed[0m [32m13.89MB[0m in [36m 0s[0m, [32m68.14MB/s[0m [1mindexed[0m [32m14.02MB[0m in [36m 0s[0m, [32m68.21MB/s[0m [1mindexed[0m [32m14.16MB[0m in [36m 0s[0m, [32m68.18MB/s[0m [1mindexed[0m [32m14.29MB[0m in [36m 0s[0m, [32m68.22MB/s[0m [1mindexed[0m [32m14.42MB[0m in [36m 0s[0m, [32m68.24MB/s[0m [1mindexed[0m [32m14.50MB[0m in [36m 0s[0m, [32m68.00MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m4.67TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m816.33TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m944.24TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m941.69TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m773.29TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.12PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m925.89TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m848.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m755.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.50PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m141.64TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.00PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m41.15TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m104.98TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m974.51TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m734.68TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m693.96TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m862.85TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.13PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m753.02TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m980.44TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m858.26TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m874.91TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m966.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m995.09TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m10.94MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m784.28TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m815.06TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.72PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m925.08TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m574.40TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m369.15TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m753.02TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m485.00TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m793.62TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m803.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m786.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.73PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.04PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.17PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m939.79TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m625.83TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m705.76TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m739.08TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.01MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m796.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.02PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.10PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m855.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.53PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m742.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.31PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.01PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m658.34TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m851.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.09PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.03PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m11.34MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m848.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m967.10TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.31PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.06PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.07PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.31PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.36PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.27PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.21PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m586.53TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.08PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.00PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m835.35TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.16PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m957.82TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.05PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m962.44TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.35PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m2.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.10PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.51PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.14PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m500.51TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m826.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.26PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m632.05TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m650.68TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.39PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m781.21TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m791.83TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m661.77TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m716.85TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.18PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.23PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.12PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m962.66TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.95PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m982.27TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.44PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m24.83MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m834.02TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m693.96TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m163.78TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.57PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m21.73MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.19PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m769.88TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m750.73TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.22PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.23PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m602.80TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m23.35MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m418.26TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m879.49TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m899.29TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.10PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m311.43TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m733.65TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m891.27TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.11PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m801.20TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m18.62MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m705.64TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m17.78MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m973.61TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.59MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m870.19TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m981.35TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m888.81TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m695.46TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.20PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m22.03MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m707.18TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m823.06TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m858.43TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.56MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.75PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m713.80TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m20.86MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m800.75TB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m1.29PB/s[0m [1mindexed[0m [32m373.00B[0m in [36m 0s[0m, [32m19.80MB/s[0m [1mindexed[0m [32m1.00TB[0m in [36m 0s[0m, [32m789.89TB/s[0m # Set as data.table and rename columns for ease of use setDT(tcga_muts) setnames(tcga_muts, &quot;Gene&quot;, &quot;gene&quot;) # Get actual patient ID tcga_muts[, ID := gsub(&quot;-0.*|-0.*&quot;, &quot;&quot;, Tumor_Sample_Barcode)] segments[, ID := gsub(&quot;-0.*&quot;, &quot;&quot;, ID)] # Select IDs that are in cn data and vice versa tcga_muts = tcga_muts[ID %in% segments$ID &amp; Variant_Classification == &quot;Missense_Mutation&quot;, .(ID, Hugo_Symbol)] tcga_muts[, alteration := &quot;SNV&quot;] setnames(tcga_muts, &quot;Hugo_Symbol&quot;, &quot;gene&quot;) # Select only IDs that are also in the mutation data.table segments = segments[ID %in% tcga_muts$ID] # Setnames setnames(segments, c(&quot;ID&quot;, &quot;chr&quot;, &quot;start&quot;, &quot;end&quot;, &quot;num_mark&quot;, &quot;segmean&quot;)) # Get deletions only and remove X from cosmic segments[segmean &lt;= -.5, alteration := &quot;Deleted&quot;] segments[segmean &gt;= .5, alteration := &quot;Amplified&quot;] segments = segments[!is.na(alteration), ] cosmic = cosmic[chr != &quot;X&quot;,] cosmic[, chr := as.integer(chr)] # Setkeys setkey(segments, chr, start, end) setkey(cosmic, chr, start, end) # Get overlap with COSMIC overlap = foverlaps(cosmic, segments) overlap = overlap[complete.cases(overlap)] # Get unique IDS overlap = unique(overlap, by = c(&quot;ID&quot;, &quot;gene&quot;)) # Merge with mutations overlap = overlap[, .(ID, alteration, gene)] overlap = rbind(overlap, tcga_muts[, .(ID, gene, alteration)]) # Select genes genes_select = c(&quot;FOXO1&quot;, &quot;FOXO3&quot;, &quot;FOXP1&quot;, &quot;RB1&quot;, &quot;CCNC&quot;, &quot;CDX2&quot;, &quot;LATS2&quot;, &quot;PRDM1&quot;, &quot;BRCA2&quot;) overlap = overlap[gene %in% genes_select, ] # Dcast result = dcast(overlap, gene ~ ID, value.var = &quot;alteration&quot;, fun.aggregate = function(x) paste(x, collapse = &quot;;&quot;)) # Prepare oncoprint cols = brewer.pal(3, &quot;Set1&quot;)[c(1, 2, 3)] names(cols) = c(&quot;Amplified&quot;, &quot;Deleted&quot;, &quot;SNV&quot;) mat = as.matrix(result[, 2:ncol(result)]) rownames(mat) = result[[1]] # Add clinical Gleason score clinical = clinical[, .(PATIENT_ID, GLEASON_SCORE, GLEASON_PATTERN_PRIMARY, GLEASON_PATTERN_SECONDARY)] clinical[, gleason := paste0(GLEASON_PATTERN_PRIMARY, &quot;+&quot;, GLEASON_PATTERN_SECONDARY)] # Assign clinical grade groups based on gleason clinical[GLEASON_SCORE == 6, grade_group := &quot;Grade Group 1&quot;] clinical[gleason == &quot;3+4&quot;, grade_group := &quot;Grade Group 2&quot;] clinical[gleason == &quot;4+3&quot;, grade_group := &quot;Grade Group 3&quot;] clinical[GLEASON_SCORE == 8, grade_group := &quot;Grade Group 4&quot;] clinical[GLEASON_SCORE &gt; 8, grade_group := &quot;Grade Group 5&quot;] # Add annotation and reorder clinical = clinical[PATIENT_ID %in% colnames(mat), ] clinical = clinical[match(colnames(mat), PATIENT_ID)] #annot_cols = list(&quot;Gleason score&quot; = c(&quot;Gleason low&quot; = brewer.pal(3, &quot;Set1&quot;)[2], &quot;Gleason high&quot; = brewer.pal(3, &quot;Set1&quot;)[1])) annot_cols = list(&quot;Grade Group&quot; = c(&quot;Grade Group 1&quot; = pal_npg()(5)[1], &quot;Grade Group 2&quot; = pal_npg()(5)[2], &quot;Grade Group 3&quot; = pal_npg()(5)[3], &quot;Grade Group 4&quot; = pal_npg()(5)[4], &quot;Grade Group 5&quot; = pal_npg()(5)[5])) onco_annot = HeatmapAnnotation(`Grade Group` = clinical$grade_group, col = annot_cols) #plot oncoPrint(mat, alter_fun = list( background = alter_graphic(&quot;rect&quot;, width = 0.9, height = 0.9, fill = &quot;#FFFFFF&quot;, col = &quot;black&quot;, size = .1), Amplified = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.9, fill = cols[&quot;Amplified&quot;]), Deleted = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.9, fill = cols[&quot;Deleted&quot;]), SNV = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.45, fill = cols[&quot;SNV&quot;])), col = cols, border = &quot;black&quot;, show_column_names = F, show_row_names = T, remove_empty_rows = T, show_pct = F, row_names_gp = gpar(fontsize = 10), top_annotation = NULL, bottom_annotation = onco_annot) 18.4 Targeted sequencing mutations Finally, we also plot the mutation data of certain genes from our targeted deep sequencing. muts = fread(&quot;./data/mutations/P3_filtered_SNVs.tsv&quot;) annot = fread(&quot;./annotation/P3.tsv&quot;, header = FALSE, col.names = c(&quot;library&quot;, &quot;section&quot;, &quot;pathology&quot;)) # Remove underscore in sample muts[, SAMPLE := gsub(&quot;_&quot;, &quot;&quot;, SAMPLE)] # Remove LOC/2nd genes muts[, GeneName := gsub(&quot;LOC.*:&quot;, &quot;&quot;, GeneName)] muts[, GeneName := gsub(&quot;:.*&quot;, &quot;&quot;, GeneName)] # Get number of mutations num_muts = muts[, .N, by = SAMPLE] num_muts[, SAMPLE := gsub(&quot;P.&quot;, &quot;&quot;, SAMPLE)] setnames(num_muts, c(&quot;section&quot;, &quot;n_muts&quot;)) # Merge with annotation num_muts = merge(num_muts, annot, by = &quot;section&quot;) # Add coordinates num_muts[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] num_muts[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] # Fill dt fill_dt = data.table(section = annot[!section %in% num_muts$section, section], n_muts = NA, library = NA, pathology = NA) fill_dt[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, section))] fill_dt[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, section))] num_muts = rbind(num_muts, fill_dt) # Plot gene specific heatmaps genelist = c(&quot;LRP1B&quot;, &quot;SPTA1&quot;, &quot;SPOP&quot;, &quot;FOXA1&quot;, &quot;FOXP1&quot;) lapply(genelist, function(gene) { dt = muts[GeneName == gene, .(count = .N), by = .(SAMPLE)] # Add sections dt = rbind(dt, num_muts[, .(SAMPLE = section, count = NA)]) dt = dt[, .(count = sum(count, na.rm = T)), by = SAMPLE] dt[count == 0, count := NA] # Add coordinates dt[, x := as.numeric(gsub(&quot;L|C.&quot;, &quot;&quot;, SAMPLE))] dt[, y := as.numeric(gsub(&quot;L.|C&quot;, &quot;&quot;, SAMPLE))] # Plot ggplot(dt, aes(x = x, y = y, fill = count)) + geom_tile() + labs(title = gene) + scale_fill_distiller(name = &quot;Number of somatic\\nnon-synonymous\\nmutations detected&quot;, palette = &quot;Reds&quot;, direction = 1, na.value = &quot;grey&quot;) + geom_hline(yintercept = seq(from = .5, to = max(dt$y), by = 1)) + geom_vline(xintercept = seq(from = .5, to = max(dt$x), by = 1)) + scale_y_reverse(expand = c(0, 0), breaks = seq(1, max(dt$y)), labels = seq(1, max(dt$y))) + scale_x_reverse(expand = c(0, 0), breaks = seq(1, max(dt$x)), labels = seq(1, max(dt$x))) + theme(axis.title = element_blank(), legend.position = &quot;none&quot;) }) ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] "],["cosmic-deletions-and-targeted-panel-mutations.html", "19 COSMIC deletions and targeted panel mutations 19.1 TRR/FER specific subclones 19.2 Variant allele frequency SNVs", " 19 COSMIC deletions and targeted panel mutations This sections produces all the figures used in Supplementary Figure 19. # Source setup file source(&quot;./functions/setup.R&quot;) 19.1 TRR/FER specific subclones First we look at subclones (identified based on copynumber profiles previously) that are exclusively present in Tumour-Rich Regions (TRRs) or Focally Enriched regions (FERs). For these subclones we plot the genes that are altered. The spatial distributions of cells from these subclones can be found in the plots created previously (scCUTseq_subclone-distributions.Rmd). profiles = fread(&quot;./data/subclones/P6_median_cn.tsv&quot;) clones = fread(&quot;./data/subclones/P6_clones.tsv&quot;) cosmic = fread(&quot;./data/genelists/cosmic_gene-census.tsv&quot;) annot = fread(&quot;./annotation/P6.tsv&quot;, header = F) # Merge clones with annot clones[, library := gsub(&quot;_.*&quot;, &quot;&quot;, sample_id)] clones_annot = merge(clones, annot, by.x = &quot;library&quot;, by.y = &quot;V1&quot;) # Select clones that are only present in cancer/focal total = clones_annot[, .(paste(unique(V3), collapse = &quot;;&quot;)), by = cluster] clones_selected = total[!grepl(&quot;Normal&quot;, V1)] # Get selected profiles = profiles[cluster %in% clones_selected$cluster] profiles = profiles[(total_cn != 2 &amp; chr != &quot;X&quot;) | (total_cn != 1 &amp; chr == &quot;X&quot;)] # Get overlaps setkey(profiles, chr, start, end) setkey(cosmic, chr, start, end) overlaps = foverlaps(cosmic, profiles)[!is.na(total_cn)] overlaps[, alteration := ifelse(total_cn &lt; 2, &quot;Deleted&quot;, &quot;Amplified&quot;)] # Get unique unique_overlaps = unique(overlaps[, .(name, gene, alteration)]) # Make oncoprint unique_overlaps_wide = dcast(unique_overlaps, name ~ gene, value.var = &quot;alteration&quot;) mat = as.matrix(unique_overlaps_wide[, 2:ncol(unique_overlaps_wide)]) rownames(mat) = unique_overlaps_wide$name # Plot oncoprint # colors cols = brewer.pal(3, &quot;Set1&quot;)[c(1, 2)] names(cols) = c(&quot;Amplified&quot;, &quot;Deleted&quot;) oncoPrint(t(mat), alter_fun = list( background = alter_graphic(&quot;rect&quot;, width = 0.9, height = 0.9, fill = &quot;#FFFFFF&quot;, col = &quot;black&quot;, size = .1), Amplified = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.85, fill = cols[&quot;Amplified&quot;]), Deleted = alter_graphic(&quot;rect&quot;, width = 0.85, height = 0.85, fill = cols[&quot;Deleted&quot;])), col = cols, border = &quot;black&quot;, show_column_names = T, show_row_names = T, remove_empty_rows = T, show_pct = F, row_names_gp = gpar(fontsize = 17), column_names_gp = gpar(fontsize = 22)) 19.2 Variant allele frequency SNVs Next we wanted to see the distribution of the Variant Allele Frequencies in both of our patients. First we plot P3. # Load in SNV data snv = fread(&quot;./data/mutations/P3_filtered_SNVs.tsv&quot;) # Remove underscore in sample snv[, SAMPLE := gsub(&quot;_&quot;, &quot;&quot;, SAMPLE)] # Remove LOC/2nd genes snv[, GeneName := gsub(&quot;LOC.*:&quot;, &quot;&quot;, GeneName)] snv[, GeneName := gsub(&quot;:.*&quot;, &quot;&quot;, GeneName)] # Get VAFs muts_vaf = snv[, .(mean_vaf = mean(VAF), count = .N), by = GeneName] # Plot ggplot(muts_vaf, aes(x = &quot;&quot;, y = mean_vaf, label = GeneName, color = count)) + geom_quasirandom(size = 3) + geom_text_repel(position = position_quasirandom()) + scale_color_viridis_c(name = &quot;Number of sections\\nwith gene mutated&quot;, option = &quot;D&quot;) + labs(y = &quot;Mean VAF&quot;, x = &quot;&quot;, color = &quot;Number of sections\\nwith gene mutated&quot;) + theme(axis.ticks.x = element_blank()) Following this, we do the same for P6. # Load in SNV data snv = fread(&quot;./data/mutations/P6_filtered_SNVs.tsv&quot;) # Remove underscore in sample snv[, SAMPLE := gsub(&quot;_&quot;, &quot;&quot;, SAMPLE)] # Remove LOC/2nd genes snv[, GeneName := gsub(&quot;LOC.*:&quot;, &quot;&quot;, GeneName)] snv[, GeneName := gsub(&quot;:.*&quot;, &quot;&quot;, GeneName)] # Get VAFs muts_vaf = snv[, .(mean_vaf = mean(VAF), count = .N), by = GeneName] # Plot ggplot(muts_vaf, aes(x = &quot;&quot;, y = mean_vaf, label = GeneName, color = count)) + geom_quasirandom(size = 3) + geom_text_repel(position = position_quasirandom()) + scale_color_viridis_c(name = &quot;Number of sections\\nwith gene mutated&quot;, option = &quot;D&quot;) + labs(y = &quot;Mean VAF&quot;, x = &quot;&quot;, color = &quot;Number of sections\\nwith gene mutated&quot;) + theme(axis.ticks.x = element_blank()) "],["session-info.html", "20 Session info 20.1 Dockerfile", " 20 Session info sessionInfo() ## R version 4.3.1 (2023-06-16) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.6 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=sv_SE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=sv_SE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=sv_SE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=sv_SE.UTF-8 LC_IDENTIFICATION=C ## ## time zone: Europe/Stockholm ## tzcode source: system (glibc) ## ## attached base packages: ## [1] stats4 grid stats graphics grDevices ## [6] utils datasets methods base ## ## other attached packages: ## [1] containerit_0.6.0.9004 lubridate_1.9.2 ## [3] forcats_1.0.0 purrr_1.0.1 ## [5] tibble_3.2.1 tidyverse_2.0.0 ## [7] caret_6.0-94 lattice_0.21-8 ## [9] pROC_1.18.4 randomForest_4.7-1.1 ## [11] TCGAbiolinks_2.28.3 maftools_2.16.0 ## [13] DescTools_0.99.49 uwot_0.1.16 ## [15] Matrix_1.6-0 ineq_0.2-13 ## [17] readr_2.1.4 stringr_1.5.0 ## [19] treeio_1.24.1 GenomicRanges_1.52.0 ## [21] GenomeInfoDb_1.36.1 IRanges_2.34.1 ## [23] S4Vectors_0.38.1 BiocGenerics_0.46.0 ## [25] tidyr_1.3.0 dplyr_1.1.2 ## [27] ggrastr_1.0.2 Polychrome_1.5.1 ## [29] circlize_0.4.15 ComplexHeatmap_2.16.0 ## [31] ggrepel_0.9.3 ggbeeswarm_0.7.2 ## [33] ggtree_3.8.0 patchwork_1.1.2 ## [35] ggdendro_0.1.23 RColorBrewer_1.1-3 ## [37] viridis_0.6.3 viridisLite_0.4.2 ## [39] ggsci_3.0.0 ggthemes_4.2.4 ## [41] ggpubr_0.6.0 cowplot_1.1.1 ## [43] gridExtra_2.3 ggplot2_3.4.2 ## [45] data.table_1.14.8 ## ## loaded via a namespace (and not attached): ## [1] fs_1.6.2 ## [2] matrixStats_1.0.0 ## [3] bitops_1.0-7 ## [4] httr_1.4.6 ## [5] doParallel_1.0.17 ## [6] tools_4.3.1 ## [7] backports_1.4.1 ## [8] utf8_1.2.3 ## [9] R6_2.5.1 ## [10] lazyeval_0.2.2 ## [11] GetoptLong_1.0.5 ## [12] withr_2.5.0 ## [13] prettyunits_1.1.1 ## [14] versions_0.3 ## [15] cli_3.6.1 ## [16] Biobase_2.60.0 ## [17] formatR_1.14 ## [18] Cairo_1.6-0 ## [19] labeling_0.4.2 ## [20] sass_0.4.6 ## [21] mvtnorm_1.2-2 ## [22] proxy_0.4-27 ## [23] yulab.utils_0.0.6 ## [24] dbscan_1.1-11 ## [25] parallelly_1.36.0 ## [26] readxl_1.4.3 ## [27] rstudioapi_0.15.0 ## [28] RSQLite_2.3.1 ## [29] FNN_1.1.3.2 ## [30] generics_0.1.3 ## [31] gridGraphics_0.5-1 ## [32] shape_1.4.6 ## [33] gtools_3.9.4 ## [34] vroom_1.6.3 ## [35] car_3.1-2 ## [36] futile.logger_1.4.3 ## [37] fansi_1.0.4 ## [38] abind_1.4-5 ## [39] lifecycle_1.0.3 ## [40] scatterplot3d_0.3-44 ## [41] yaml_2.3.7 ## [42] carData_3.0-5 ## [43] SummarizedExperiment_1.30.2 ## [44] recipes_1.0.6 ## [45] BiocFileCache_2.8.0 ## [46] blob_1.2.4 ## [47] promises_1.2.0.1 ## [48] crayon_1.5.2 ## [49] miniUI_0.1.1.1 ## [50] KEGGREST_1.40.0 ## [51] pillar_1.9.0 ## [52] knitr_1.43 ## [53] rjson_0.2.21 ## [54] boot_1.3-28.1 ## [55] gld_2.6.6 ## [56] TCGAbiolinksGUI.data_1.20.0 ## [57] future.apply_1.11.0 ## [58] codetools_0.2-19 ## [59] glue_1.6.2 ## [60] downloader_0.4 ## [61] ggfun_0.1.1 ## [62] vctrs_0.6.3 ## [63] png_0.1-8 ## [64] cellranger_1.1.0 ## [65] gtable_0.3.3 ## [66] cachem_1.0.8 ## [67] gower_1.0.1 ## [68] xfun_0.39 ## [69] mime_0.12 ## [70] S4Arrays_1.0.4 ## [71] prodlim_2023.03.31 ## [72] survival_3.5-5 ## [73] timeDate_4022.108 ## [74] iterators_1.0.14 ## [75] hardhat_1.3.0 ## [76] lava_1.7.2.1 ## [77] ellipsis_0.3.2 ## [78] ipred_0.9-14 ## [79] nlme_3.1-162 ## [80] bit64_4.0.5 ## [81] RcppAnnoy_0.0.21 ## [82] progress_1.2.2 ## [83] filelock_1.0.2 ## [84] rprojroot_2.0.3 ## [85] bslib_0.5.0 ## [86] irlba_2.3.5.1 ## [87] vipor_0.4.5 ## [88] rpart_4.1.19 ## [89] colorspace_2.1-0 ## [90] DBI_1.1.3 ## [91] nnet_7.3-19 ## [92] DNAcopy_1.74.1 ## [93] Exact_3.2 ## [94] tidyselect_1.2.0 ## [95] bit_4.0.5 ## [96] compiler_4.3.1 ## [97] curl_5.0.1 ## [98] rvest_1.0.3 ## [99] expm_0.999-7 ## [100] xml2_1.3.5 ## [101] desc_1.4.2 ## [102] DelayedArray_0.26.6 ## [103] bookdown_0.34 ## [104] scales_1.2.1 ## [105] rappdirs_0.3.3 ## [106] digest_0.6.33 ## [107] rmarkdown_2.23 ## [108] XVector_0.40.0 ## [109] htmltools_0.5.5 ## [110] pkgconfig_2.0.3 ## [111] MatrixGenerics_1.12.2 ## [112] highr_0.10 ## [113] dbplyr_2.3.3 ## [114] fastmap_1.1.1 ## [115] rlang_1.1.1 ## [116] GlobalOptions_0.1.2 ## [117] shiny_1.7.4.1 ## [118] farver_2.1.1 ## [119] jquerylib_0.1.4 ## [120] jsonlite_1.8.7 ## [121] ModelMetrics_1.2.2.2 ## [122] RCurl_1.98-1.12 ## [123] magrittr_2.0.3 ## [124] GenomeInfoDbData_1.2.10 ## [125] ggplotify_0.1.1 ## [126] munsell_0.5.0 ## [127] Rcpp_1.0.11 ## [128] ape_5.7-1 ## [129] stringi_1.7.12 ## [130] rootSolve_1.8.2.3 ## [131] zlibbioc_1.46.0 ## [132] MASS_7.3-60 ## [133] plyr_1.8.8 ## [134] shinyFiles_0.9.3 ## [135] parallel_4.3.1 ## [136] listenv_0.9.0 ## [137] lmom_2.9 ## [138] semver_0.2.0 ## [139] Biostrings_2.68.1 ## [140] splines_4.3.1 ## [141] hms_1.1.3 ## [142] ggsignif_0.6.4 ## [143] reshape2_1.4.4 ## [144] biomaRt_2.56.1 ## [145] futile.options_1.0.1 ## [146] XML_3.99-0.14 ## [147] evaluate_0.21 ## [148] stevedore_0.9.6 ## [149] BiocManager_1.30.21 ## [150] lambda.r_1.2.4 ## [151] httpuv_1.6.11 ## [152] tzdb_0.4.0 ## [153] foreach_1.5.2 ## [154] future_1.33.0 ## [155] clue_0.3-64 ## [156] xtable_1.8-4 ## [157] broom_1.0.5 ## [158] e1071_1.7-13 ## [159] tidytree_0.4.2 ## [160] later_1.3.1 ## [161] rstatix_0.7.2 ## [162] class_7.3-22 ## [163] aplot_0.1.10 ## [164] memoise_2.0.1 ## [165] beeswarm_0.4.0 ## [166] AnnotationDbi_1.62.2 ## [167] cluster_2.1.4 ## [168] timechange_0.2.0 ## [169] globals_0.16.2 20.1 Dockerfile require(containerit) dockerfile = dockerfile(from = utils::sessionInfo(), versioned_libs = TRUE, versioned_packages = TRUE) ## INFO [2023-07-13 16:10:16] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:16] Trying to determine system requirements for the package(s) &#39;BiocGenerics&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:17] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:17] Trying to determine system requirements for the package(s) &#39;caret&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:18] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:18] Trying to determine system requirements for the package(s) &#39;circlize&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:19] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:19] Trying to determine system requirements for the package(s) &#39;ComplexHeatmap&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:20] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:20] Trying to determine system requirements for the package(s) &#39;cowplot&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:20] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:20] Trying to determine system requirements for the package(s) &#39;data.table&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:21] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:21] Trying to determine system requirements for the package(s) &#39;DescTools&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:22] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:22] Trying to determine system requirements for the package(s) &#39;dplyr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:23] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:23] Trying to determine system requirements for the package(s) &#39;forcats&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:24] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:24] Trying to determine system requirements for the package(s) &#39;GenomeInfoDb&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:25] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:25] Trying to determine system requirements for the package(s) &#39;GenomicRanges&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:26] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:26] Trying to determine system requirements for the package(s) &#39;ggbeeswarm&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:26] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:26] Trying to determine system requirements for the package(s) &#39;ggdendro&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:27] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:27] Trying to determine system requirements for the package(s) &#39;ggplot2&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:28] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:28] Trying to determine system requirements for the package(s) &#39;ggpubr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:29] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:29] Trying to determine system requirements for the package(s) &#39;ggrastr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:30] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:30] Trying to determine system requirements for the package(s) &#39;ggrepel&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:31] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:31] Trying to determine system requirements for the package(s) &#39;ggsci&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:32] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:32] Trying to determine system requirements for the package(s) &#39;ggthemes&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:32] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:32] Trying to determine system requirements for the package(s) &#39;ggtree&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:33] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:33] Trying to determine system requirements for the package(s) &#39;gridExtra&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:34] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:34] Trying to determine system requirements for the package(s) &#39;ineq&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:35] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:35] Trying to determine system requirements for the package(s) &#39;IRanges&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:36] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:36] Trying to determine system requirements for the package(s) &#39;lattice&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:37] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:37] Trying to determine system requirements for the package(s) &#39;lubridate&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:38] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:38] Trying to determine system requirements for the package(s) &#39;maftools&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:38] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:38] Trying to determine system requirements for the package(s) &#39;Matrix&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:39] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:39] Trying to determine system requirements for the package(s) &#39;patchwork&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:40] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:40] Trying to determine system requirements for the package(s) &#39;Polychrome&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:41] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:41] Trying to determine system requirements for the package(s) &#39;pROC&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:42] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:42] Trying to determine system requirements for the package(s) &#39;purrr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:43] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:43] Trying to determine system requirements for the package(s) &#39;randomForest&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:44] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:44] Trying to determine system requirements for the package(s) &#39;RColorBrewer&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:44] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:44] Trying to determine system requirements for the package(s) &#39;readr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:45] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:45] Trying to determine system requirements for the package(s) &#39;S4Vectors&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:46] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:46] Trying to determine system requirements for the package(s) &#39;stringr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:47] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:47] Trying to determine system requirements for the package(s) &#39;TCGAbiolinks&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:48] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:48] Trying to determine system requirements for the package(s) &#39;tibble&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:49] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:49] Trying to determine system requirements for the package(s) &#39;tidyr&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:50] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:50] Trying to determine system requirements for the package(s) &#39;tidyverse&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:50] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:50] Trying to determine system requirements for the package(s) &#39;treeio&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:51] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:51] Trying to determine system requirements for the package(s) &#39;uwot&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:52] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:52] Trying to determine system requirements for the package(s) &#39;viridis&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:53] Going online? TRUE ... to retrieve system dependencies (sysreq-api) ## INFO [2023-07-13 16:10:53] Trying to determine system requirements for the package(s) &#39;viridisLite&#39; from sysreqs online DB ## INFO [2023-07-13 16:10:54] Versioned packages enabled, installing &#39;versions&#39; ## INFO [2023-07-13 16:10:54] Adding versioned CRAN packages: lubridate, forcats, purrr, tibble, tidyverse, caret, lattice, pROC, randomForest, DescTools, uwot, Matrix, ineq, readr, stringr, tidyr, dplyr, ggrastr, Polychrome, circlize, ggrepel, ggbeeswarm, patchwork, ggdendro, RColorBrewer, viridis, viridisLite, ggsci, ggthemes, ggpubr, cowplot, gridExtra, ggplot2, data.table ## WARN [2023-07-13 16:10:54] Adding versioned Bioconductor packages not supported: TCGAbiolinks, maftools, treeio, GenomicRanges, GenomeInfoDb, IRanges, S4Vectors, BiocGenerics, ComplexHeatmap, ggtree ## INFO [2023-07-13 16:10:54] Adding Bioconductor packages: BiocGenerics, ComplexHeatmap, GenomeInfoDb, GenomicRanges, ggtree, IRanges, maftools, S4Vectors, TCGAbiolinks, treeio ## INFO [2023-07-13 16:10:54] Created Dockerfile-Object based on sessionInfo print(dockerfile) ## FROM rocker/r-ver:4.3.1 ## LABEL maintainer=&quot;luukharbers&quot; ## RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \\ ## &amp;&amp; apt-get install -y zlib1g-dev ## RUN [&quot;install2.r&quot;, &quot;versions&quot;] ## RUN [&quot;Rscript&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;caret&#39;, &#39;6.0-94&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;circlize&#39;, &#39;0.4.15&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;cowplot&#39;, &#39;1.1.1&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;data.table&#39;, &#39;1.14.8&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;DescTools&#39;, &#39;0.99.49&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;dplyr&#39;, &#39;1.1.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;forcats&#39;, &#39;1.0.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggbeeswarm&#39;, &#39;0.7.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggdendro&#39;, &#39;0.1.23&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggplot2&#39;, &#39;3.4.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggpubr&#39;, &#39;0.6.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggrastr&#39;, &#39;1.0.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggrepel&#39;, &#39;0.9.3&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggsci&#39;, &#39;3.0.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ggthemes&#39;, &#39;4.2.4&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;gridExtra&#39;, &#39;2.3&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;ineq&#39;, &#39;0.2-13&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;lattice&#39;, &#39;0.21-8&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;lubridate&#39;, &#39;1.9.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;Matrix&#39;, &#39;1.6-0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;patchwork&#39;, &#39;1.1.2&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;Polychrome&#39;, &#39;1.5.1&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;pROC&#39;, &#39;1.18.4&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;purrr&#39;, &#39;1.0.1&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;randomForest&#39;, &#39;4.7-1.1&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;RColorBrewer&#39;, &#39;1.1-3&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;readr&#39;, &#39;2.1.4&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;stringr&#39;, &#39;1.5.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;tibble&#39;, &#39;3.2.1&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;tidyr&#39;, &#39;1.3.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;tidyverse&#39;, &#39;2.0.0&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;uwot&#39;, &#39;0.1.16&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;viridis&#39;, &#39;0.6.3&#39;)&quot;, &quot;-e&quot;, &quot;versions::install.versions(&#39;viridisLite&#39;, &#39;0.4.2&#39;)&quot;] ## RUN [&quot;install2.r&quot;, &quot;-r https://bioconductor.org/packages/3.17/bioc -r https://bioconductor.org/packages/3.17/data/annotation -r https://bioconductor.org/packages/3.17/data/experiment -r https://bioconductor.org/packages/3.17/workflows&quot;, &quot;BiocGenerics&quot;, &quot;ComplexHeatmap&quot;, &quot;GenomeInfoDb&quot;, &quot;GenomicRanges&quot;, &quot;ggtree&quot;, &quot;IRanges&quot;, &quot;maftools&quot;, &quot;S4Vectors&quot;, &quot;TCGAbiolinks&quot;, &quot;treeio&quot;] ## WORKDIR /payload/ ## CMD [&quot;R&quot;] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
